const express = require('express');
const http = require('http');
const https = require('https');
const fs = require('fs');
const socketIo = require('socket.io');
const dgram = require('dgram');
const path = require('path');
const { v4: uuidv4 } = require('uuid');
require('dotenv').config({ path: '.env.externalmedia' });

// Import translation services
const { createClient } = require('@deepgram/sdk');
const deepl = require('deepl-node');
// const sdk = require('microsoft-cognitiveservices-speech-sdk'); // Replaced with ElevenLabs
const ElevenLabsTTSService = require('./elevenlabs-tts-service');
const HumeStreamingClient = require('./hume-streaming-client');

// Import HMLCP modules
const { UserProfile, ULOLayer, PatternExtractor } = require('./hmlcp');
const { applyDefaultProfile } = require('./hmlcp/default-profiles');


const app = express();

// Create HTTPS server if certificates exist, otherwise HTTP
// But always use HTTP in Azure App Service (Azure handles SSL termination)
let server;
const isAzure = !!process.env.WEBSITE_INSTANCE_ID;

if (isAzure) {
  // Azure App Service - use HTTP (Azure handles HTTPS)
  server = http.createServer(app);
  console.log('‚úì HTTP server for Azure App Service (Azure handles HTTPS termination)');
} else {
  try {
    const certPath = path.join(__dirname, 'cert.pem');
    const keyPath = path.join(__dirname, 'key.pem');

    if (fs.existsSync(certPath) && fs.existsSync(keyPath)) {
      const options = {
        key: fs.readFileSync(keyPath),
        cert: fs.readFileSync(certPath)
      };
      server = https.createServer(options, app);
      console.log('‚úì HTTPS server configured with SSL certificates');
    } else {
      server = http.createServer(app);
      console.log('‚ö† HTTP server (certificates not found)');
    }
  } catch (error) {
    console.error('Error loading certificates, falling back to HTTP:', error.message);
    server = http.createServer(app);
  }
}

const io = socketIo(server, {
  cors: {
    origin: "*",
    methods: ["GET", "POST"]
  },
  maxHttpBufferSize: 1e8 // 100 MB for audio chunks
});

// Make Socket.IO available globally for audiosocket-integration
global.io = io;

// ========================================================================
// OLD TIMING CLIENT - DISABLED 2025-11-12
// ========================================================================
// This external timing server on port 6000 is replaced by embedded timing model
// OLD Architecture: Conf Server ‚Üí Timing Server (6000) ‚Üí manipulate ‚Üí back to Conf Server ‚Üí Asterisk
// NEW Architecture: Conf Server ‚Üí directly to Asterisk (parallel mode)
// ========================================================================
/*
const TimingClient = require('./timing-client');
global.timingClient = new TimingClient();
global.timingClient.connect().then(() => {
    console.log('[Server] ‚úì Timing client connected');
}).catch(err => {
    console.error('[Server] ‚úó Timing client connection failed:', err.message);
});
*/
console.log('[Server] ‚Ñπ OLD Timing client disabled - using embedded timing model');

// Phase 2: Global session registry for audio injection by extension
// Key: extension number (string), Value: session object
global.activeSessions = new Map();
console.log('[Phase2] Global session registry initialized');

// [DISABLED FOR 9007/9008] // Start AudioSocket server (for Asterisk integration on port 5050)
// [DISABLED FOR 9007/9008] // IMPORTANT: Must load AFTER global.io is set
// [DISABLED FOR 9007/9008] require("./audiosocket-integration");

// ========================================================================
// OLD INJECT_AUDIO HANDLER - DISABLED 2025-11-12
// ========================================================================
// This was part of the OLD timing server architecture
// Now replaced by direct Gateway ‚Üí Conference Server communication
// ========================================================================
/*
global.timingClient.setInjectAudioHandler((msg) => {
    const { toExtension, audioData, timestamp } = msg;

    // Look up session by extension
    const session = global.activeSessions.get(String(toExtension));

    if (!session) {
        console.warn(`[Phase2] ‚úó No session found for extension ${toExtension}`);
        return;
    }

    if (!session.micWebSocket || session.micWebSocket.readyState !== 1) {
        console.warn(`[Phase2] ‚úó MicWebSocket not ready for extension ${toExtension}`);
        return;
    }

    // Decode base64 audio to buffer
    const audioBuffer = Buffer.from(audioData, 'base64');

    // Inject audio using the global function
    if (global.sendAudioToMicEndpoint) {
        global.sendAudioToMicEndpoint(session.micWebSocket, audioBuffer);
        console.log(`[Phase2] ‚úì Injected ${audioBuffer.length} bytes to extension ${toExtension}`);
    } else {
        console.error('[Phase2] ‚úó sendAudioToMicEndpoint not available');
    }
});
console.log('[Phase2] INJECT_AUDIO handler registered');
*/
console.log('[Phase2] ‚Ñπ OLD INJECT_AUDIO handler disabled - using direct Gateway communication');

// Latency Control Backend (for testing UI only - does not affect production)
// const LatencyControlBackend = require('./latency-control-backend');
// const latencyControl = new LatencyControlBackend();
// latencyControl.registerSocketHandlers(io);
console.log('[Server] ‚úì Latency Control Backend initialized (testing mode)');

app.use(express.static(path.join(__dirname, 'public')));

// Serve file directories
app.use('/files/recordings', express.static(path.join(__dirname, 'recordings')));
app.use('/files/transcripts', express.static(path.join(__dirname, 'transcripts')));
app.use('/files/translations', express.static(path.join(__dirname, 'translations')));

// Initialize services
const deepgramApiKey = process.env.DEEPGRAM_API_KEY;
const deeplApiKey = process.env.DEEPL_API_KEY;
const elevenlabsApiKey = process.env.ELEVENLABS_API_KEY;
const elevenlabsVoiceId = process.env.ELEVENLABS_DEFAULT_VOICE_ID;
const humeApiKey = process.env.HUME_EVI_API_KEY;

// Initialize DeepL translator
let translator;
if (deeplApiKey) {
  translator = new deepl.Translator(deeplApiKey);
}

// Initialize ElevenLabs TTS
let elevenlabsTTS = null;
if (elevenlabsApiKey) {
  elevenlabsTTS = new ElevenLabsTTSService(elevenlabsApiKey);
  console.log('ElevenLabs TTS service initialized');
}

// ============================================================================
// AUDIO TEST MODE - For calibrating RTP audio format without live translation
// ============================================================================
// Test audio files (PCM16, 16kHz, mono) for format testing
const TEST_AUDIO_FILES = {
  '9007': path.join(__dirname, 'test-audio-en.pcm'),  // English test tone (440 Hz)
  '9008': path.join(__dirname, 'test-audio-fr.pcm')   // French test tone (550 Hz)
};

// Test mode state
const testModeState = {
  enabled: false,
  targetExtension: null,  // Which extension to send test audio to
  interval: null,         // setInterval reference for looping
  testAudioBuffer: null,  // Loaded test audio data
  loopCount: 0            // Track how many loops have been sent
};

/**
 * Load test audio file for the target extension
 * @param {string} targetExtension - Extension to send test audio to ('9007' or '9008')
 * @returns {Buffer|null} - PCM16 audio buffer or null if file not found
 */
function loadTestAudio(targetExtension) {
  const filePath = TEST_AUDIO_FILES[targetExtension];
  if (!filePath) {
    console.error(`[Test Mode] ‚úó No test audio file configured for extension ${targetExtension}`);
    return null;
  }

  try {
    if (!fs.existsSync(filePath)) {
      console.error(`[Test Mode] ‚úó Test audio file not found: ${filePath}`);
      return null;
    }

    const audioBuffer = fs.readFileSync(filePath);
    console.log(`[Test Mode] ‚úì Loaded test audio: ${filePath} (${audioBuffer.length} bytes, ~${(audioBuffer.length / 32000).toFixed(2)}s)`);
    return audioBuffer;
  } catch (error) {
    console.error(`[Test Mode] ‚úó Error loading test audio:`, error.message);
    return null;
  }
}

/**
 * Start test mode - sends test audio in a continuous loop to target extension
 * @param {string} targetExtension - Extension to send test audio to ('9007' or '9008')
 */
function startTestMode(targetExtension) {
  if (testModeState.enabled) {
    console.log(`[Test Mode] ‚ö† Test mode already running for extension ${testModeState.targetExtension}`);
    return { success: false, message: 'Test mode already running' };
  }

  // Load test audio file
  const audioBuffer = loadTestAudio(targetExtension);
  if (!audioBuffer) {
    return { success: false, message: 'Failed to load test audio file' };
  }

  testModeState.enabled = true;
  testModeState.targetExtension = targetExtension;
  testModeState.testAudioBuffer = audioBuffer;
  testModeState.loopCount = 0;

  // Send test audio immediately, then every 4 seconds (allowing 3s audio + 1s gap)
  const sendTestAudio = () => {
    if (!testModeState.enabled) return;

    testModeState.loopCount++;
    console.log(`[Test Mode] ‚Üí Sending test audio to extension ${targetExtension} (loop #${testModeState.loopCount})`);

    // Emit translatedAudio event to Gateway (same format as real translation)
    global.io.emit('translatedAudio', {
      extension: String(targetExtension),
      audio: testModeState.testAudioBuffer,
      format: 'pcm16',
      sampleRate: 16000,
      channels: 1,
      timestamp: Date.now(),
      bufferApplied: 0,           // No buffer in test mode
      sourceExtension: 'TEST',    // Indicate test mode source
      testMode: true              // Flag to identify test audio
    });
  };

  // Send first test audio immediately
  sendTestAudio();

  // Then send every 4 seconds in a loop
  testModeState.interval = setInterval(sendTestAudio, 4000);

  console.log(`[Test Mode] ‚úì Started test mode for extension ${targetExtension} (looping every 4s)`);
  return { success: true, message: `Test mode started for extension ${targetExtension}` };
}

/**
 * Stop test mode - stops the test audio loop
 */
function stopTestMode() {
  if (!testModeState.enabled) {
    console.log(`[Test Mode] ‚ö† Test mode not running`);
    return { success: false, message: 'Test mode not running' };
  }

  if (testModeState.interval) {
    clearInterval(testModeState.interval);
    testModeState.interval = null;
  }

  const prevExtension = testModeState.targetExtension;
  const totalLoops = testModeState.loopCount;

  testModeState.enabled = false;
  testModeState.targetExtension = null;
  testModeState.testAudioBuffer = null;
  testModeState.loopCount = 0;

  console.log(`[Test Mode] ‚úì Stopped test mode (was running for extension ${prevExtension}, sent ${totalLoops} loops)`);
  return { success: true, message: `Test mode stopped (${totalLoops} loops sent)` };
}

/**
 * Get current test mode status
 * @returns {object} - Test mode status object
 */
function getTestModeStatus() {
  return {
    enabled: testModeState.enabled,
    targetExtension: testModeState.targetExtension,
    loopCount: testModeState.loopCount,
    audioLoaded: !!testModeState.testAudioBuffer
  };
}

// ============================================================================
// TIMING & BUFFERING MODULE - Step 1: Class Definitions (NOT YET ACTIVE)
// ============================================================================
// These classes are added but not yet integrated into the pipeline
// System behavior remains unchanged until Step 2

const net = require('net');

// Class 1: ExtensionPairManager - Tracks which extensions are paired
class ExtensionPairManager {
  constructor() {
    this.pairs = new Map(); // ext ‚Üí pairedExt
    this.startTimes = new Map(); // ext ‚Üí callStartTime
  }

  registerPair(ext1, ext2) {
    this.pairs.set(ext1, ext2);
    this.pairs.set(ext2, ext1);
    this.startTimes.set(ext1, Date.now());
    this.startTimes.set(ext2, Date.now());
    console.log('[PairManager] Registered pair: ' + ext1 + ' ‚Üî ' + ext2);
  }

  isPaired(ext) {
    return this.pairs.has(ext);
  }

  getPairedExtension(ext) {
    return this.pairs.get(ext);
  }

  unregisterPair(ext) {
    const paired = this.pairs.get(ext);
    if (paired) {
      this.pairs.delete(ext);
      this.pairs.delete(paired);
      this.startTimes.delete(ext);
      this.startTimes.delete(paired);
      console.log('[PairManager] Unregistered pair: ' + ext + ' ‚Üî ' + paired);
    }
  }
}

// Class 2: LatencyTracker - Tracks latency samples and calculates rolling averages
class LatencyTracker {
  constructor() {
    this.directionSamples = new Map(); // 'ext1‚Üíext2' ‚Üí [sample1, sample2, ...]
    this.stageSamples = new Map(); // 'ext:stage' ‚Üí [sample1, sample2, ...]
    this.maxSamples = 10; // Rolling average window
  }

  updateLatency(direction, latencyMs) {
    if (!this.directionSamples.has(direction)) {
      this.directionSamples.set(direction, []);
    }

    const samples = this.directionSamples.get(direction);
    samples.push(latencyMs);

    // Keep only last 10 samples
    if (samples.length > this.maxSamples) {
      samples.shift();
    }

    const avg = this.getAverageLatency(direction);
    console.log('[Latency] ' + direction + ' = ' + latencyMs + 'ms (avg: ' + Math.round(avg) + 'ms, n=' + samples.length + ')');
  }

  updateStageLatency(extension, stageName, latencyMs) {
    const key = extension + ':' + stageName;
    if (!this.stageSamples.has(key)) {
      this.stageSamples.set(key, []);
    }

    const samples = this.stageSamples.get(key);
    samples.push(latencyMs);

    if (samples.length > this.maxSamples) {
      samples.shift();
    }
  }

  getAverageLatency(direction) {
    const samples = this.directionSamples.get(direction) || [];
    if (samples.length === 0) return 0;
    return samples.reduce((a, b) => a + b) / samples.length;
  }

  getStageAverage(extension, stageName) {
    const key = extension + ':' + stageName;
    const samples = this.stageSamples.get(key) || [];
    if (samples.length === 0) return 0;
    return samples.reduce((a, b) => a + b) / samples.length;
  }

  getLatestLatency(direction) {
    const samples = this.directionSamples.get(direction) || [];
    if (samples.length === 0) return 0;
    return samples[samples.length - 1]; // Return most recent sample
  }

  getLatencyDifference(ext1, ext2) {
    const direction1 = ext1 + '‚Üí' + ext2;
    const direction2 = ext2 + '‚Üí' + ext1;

    const avg1 = this.getAverageLatency(direction1);
    const avg2 = this.getAverageLatency(direction2);

    const difference = avg1 - avg2;

    console.log('[LatencyDiff] ' + direction1 + '=' + Math.round(avg1) + 'ms, ' + direction2 + '=' + Math.round(avg2) + 'ms, Œî=' + Math.round(difference) + 'ms');

    return difference;
  }

  getCurrentLatencyDifference(ext1, ext2) {
    const direction1 = ext1 + '‚Üí' + ext2;
    const direction2 = ext2 + '‚Üí' + ext1;

    const current1 = this.getLatestLatency(direction1);
    const current2 = this.getLatestLatency(direction2);

    // Only calculate difference if BOTH extensions have valid data (non-zero)
    if (current1 === 0 || current2 === 0) {
      console.log('[LatencyDiff-Current] Skipping calculation - one or both extensions have no data yet (' + direction1 + '=' + Math.round(current1) + 'ms, ' + direction2 + '=' + Math.round(current2) + 'ms)');
      return null;
    }

    // FIXED: Inverted formula - ext2‚Üíext1 minus ext1‚Üíext2
    // Positive = ext1 is slower (needs more delay), Negative = ext1 is faster (needs less delay)
    const difference = current2 - current1;

    console.log('[LatencyDiff-Current] ' + direction1 + '=' + Math.round(current1) + 'ms, ' + direction2 + '=' + Math.round(current2) + 'ms, Œî=' + Math.round(difference) + 'ms');

    return difference;
  }

  getAllLatencies(extension) {
    const stages = [
      'audiosocket_to_asr', 'asr', 'asr_to_mt', 'mt', 'mt_to_tts',
      'tts', 'tts_to_ls', 'ls', 'ls_to_bridge'
    ];

    const result = {};
    for (const stage of stages) {
      const key = extension + ':' + stage;
      const samples = this.stageSamples.get(key) || [];
      result[stage] = {
        current: samples[samples.length - 1] || 0,
        avg: this.getStageAverage(extension, stage)
      };
    }

    return result;
  }
}

// Class 3: AudioBufferManager - Manages setTimeout-based audio buffering
class AudioBufferManager {
  constructor() {
    this.pendingBuffers = new Map(); // extension ‚Üí [{audio, timer, targetTime}, ...]
  }

  bufferAndSend(extension, audioData, delayMs, sendCallback) {
    if (delayMs === 0) {
      // No delay needed, send immediately
      sendCallback(extension, audioData);
      return;
    }

    const targetTime = Date.now() + delayMs;

    console.log('[AudioBuffer] Buffering ' + audioData.length + ' bytes for ' + extension + ' by ' + delayMs + 'ms');

    const timer = setTimeout(() => {
      console.log('[AudioBuffer] Sending buffered audio for ' + extension + ' (delayed by ' + delayMs + 'ms)');
      sendCallback(extension, audioData);

      // Remove from pending buffers
      const buffers = this.pendingBuffers.get(extension) || [];
      const index = buffers.findIndex(b => b.targetTime === targetTime);
      if (index !== -1) {
        buffers.splice(index, 1);
      }
    }, delayMs);

    // Track pending buffer
    if (!this.pendingBuffers.has(extension)) {
      this.pendingBuffers.set(extension, []);
    }

    this.pendingBuffers.get(extension).push({
      audio: audioData,
      timer,
      targetTime,
      delayMs
    });
  }

  clearBuffer(extension) {
    const buffers = this.pendingBuffers.get(extension) || [];
    buffers.forEach(b => clearTimeout(b.timer));
    this.pendingBuffers.delete(extension);
    console.log('[AudioBuffer] Cleared all pending buffers for ' + extension);
  }

  getPendingBuffers(extension) {
    return this.pendingBuffers.get(extension) || [];
  }
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// STEP 4.3: MP3 ‚Üí PCM16 AUDIO CONVERSION FUNCTION
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// Note: fs and path already declared at top of file
const { exec } = require('child_process');
const { promisify } = require('util');
const execPromise = promisify(exec);

async function convertMp3ToPcm16(mp3Buffer) {
  const tempDir = '/tmp';
  const inputFile = path.join(tempDir, `tts_${Date.now()}_${Math.random().toString(36).substr(2, 9)}.mp3`);
  const outputFile = path.join(tempDir, `tts_${Date.now()}_${Math.random().toString(36).substr(2, 9)}.pcm`);

  try {
    // Write MP3 to temporary file
    fs.writeFileSync(inputFile, mp3Buffer);

    // Convert: MP3 ‚Üí PCM16, 16kHz, mono, signed 16-bit little-endian
    const ffmpegCommand = `ffmpeg -i ${inputFile} -f s16le -acodec pcm_s16le -ar 16000 -ac 1 ${outputFile}`;
    await execPromise(ffmpegCommand);

    // Read PCM data
    const pcmBuffer = fs.readFileSync(outputFile);

    // Cleanup
    fs.unlinkSync(inputFile);
    fs.unlinkSync(outputFile);

    console.log(`[Audio Convert] MP3 ‚Üí PCM16: ${mp3Buffer.length} bytes ‚Üí ${pcmBuffer.length} bytes`);
    return pcmBuffer;

  } catch (error) {
    console.error('[Audio Convert] Error converting MP3 to PCM16:', error);
    // Cleanup on error
    try { fs.unlinkSync(inputFile); } catch {}
    try { fs.unlinkSync(outputFile); } catch {}
    throw error;
  }
}

// Class 4: DashboardTCPAPI - TCP server for dashboard metrics
class DashboardTCPAPI {
  constructor() {
    this.server = null;
    this.clients = new Map(); // socket ‚Üí { id, subscriptions: Set }
    this.nextClientId = 1;
    this.heartbeatInterval = null;
  }

  startServer(port = 6211) {
    this.server = net.createServer((socket) => {
      this.handleClientConnection(socket);
    });

    this.server.listen(port, () => {
      console.log('[TCP API] Dashboard metrics API listening on port ' + port);
    });

    // Start heartbeat
    this.heartbeatInterval = setInterval(() => {
      this.broadcastHeartbeat();
    }, 30000);
  }

  handleClientConnection(socket) {
    const clientId = this.nextClientId++;
    const clientInfo = {
      id: clientId,
      subscriptions: new Set(), // Set of extension numbers
      address: socket.remoteAddress
    };

    this.clients.set(socket, clientInfo);

    console.log('[TCP API] Client ' + clientId + ' connected from ' + socket.remoteAddress);

    // Send connection confirmation
    this.sendToClient(socket, {
      type: 'CONNECTED',
      timestamp: Date.now(),
      serverVersion: '1.0.0',
      clientId: clientId
    });

    // Handle incoming data
    let buffer = '';
    socket.on('data', (data) => {
      buffer += data.toString('utf8');

      // Process complete messages (newline-delimited)
      const lines = buffer.split('\n');
      buffer = lines.pop(); // Keep incomplete line in buffer

      for (const line of lines) {
        if (line.trim()) {
          try {
            const message = JSON.parse(line);
            this.handleClientMessage(socket, clientInfo, message);
          } catch (error) {
            console.error('[TCP API] Invalid JSON from client ' + clientId + ':', error);
          }
        }
      }
    });

    socket.on('error', (error) => {
      console.error('[TCP API] Client ' + clientId + ' error:', error);
    });

    socket.on('close', () => {
      console.log('[TCP API] Client ' + clientId + ' disconnected');
      this.clients.delete(socket);
    });
  }

  handleClientMessage(socket, clientInfo, message) {
    switch (message.type) {
      case 'SUBSCRIBE':
        if (message.extension) {
          clientInfo.subscriptions.add(message.extension);
          console.log('[TCP API] Client ' + clientInfo.id + ' subscribed to extension ' + message.extension);
        }
        break;

      case 'UNSUBSCRIBE':
        if (message.extension) {
          clientInfo.subscriptions.delete(message.extension);
          console.log('[TCP API] Client ' + clientInfo.id + ' unsubscribed from extension ' + message.extension);
        }
        break;

      default:
        console.warn('[TCP API] Unknown message type from client ' + clientInfo.id + ':', message.type);
    }
  }

  sendToClient(socket, data) {
    if (socket.writable) {
      const json = JSON.stringify(data) + '\n';
      socket.write(json);
    }
  }

  broadcastLatencyUpdate(data) {
    const message = {
      type: 'LATENCY_UPDATE',
      timestamp: Date.now(),
      ...data
    };

    // Send to subscribed clients
    for (const [socket, clientInfo] of this.clients.entries()) {
      if (clientInfo.subscriptions.size === 0 ||
          clientInfo.subscriptions.has(data.extension)) {
        this.sendToClient(socket, message);
      }
    }
  }

  broadcastStage(extension, stage, duration, stageName) {
    const message = {
      type: 'STAGE_TIMING',
      timestamp: Date.now(),
      extension,
      stage,
      duration,
      stageName
    };

    for (const [socket, clientInfo] of this.clients.entries()) {
      if (clientInfo.subscriptions.size === 0 ||
          clientInfo.subscriptions.has(extension)) {
        this.sendToClient(socket, message);
      }
    }
  }

  broadcastBufferApplied(extension, delayMs, reason, latencyDifference) {
    const message = {
      type: 'BUFFER_APPLIED',
      timestamp: Date.now(),
      extension,
      delayMs,
      reason,
      latencyDifference
    };

    for (const [socket, clientInfo] of this.clients.entries()) {
      if (clientInfo.subscriptions.size === 0 ||
          clientInfo.subscriptions.has(extension)) {
        this.sendToClient(socket, message);
      }
    }
  }

  broadcastBufferCalculation(extension, targetBufferMs, pairedExtension, latencyDifference) {
    const message = {
      type: 'BUFFER_CALCULATION',
      timestamp: Date.now(),
      extension: extension,
      targetBufferMs: Math.round(targetBufferMs),
      pairedExtension: pairedExtension,
      latencyDifference: Math.round(latencyDifference),
      status: 'calculated_not_applied'
    };

    for (const [socket, clientInfo] of this.clients.entries()) {
      if (clientInfo.subscriptions.size === 0 ||
          clientInfo.subscriptions.has(extension)) {
        this.sendToClient(socket, message);
      }
    }

    console.log('[TCP API] Broadcast buffer calculation for ' + extension + ': target=' + Math.round(targetBufferMs) + 'ms, diff=' + Math.round(latencyDifference) + 'ms');
  }

  broadcastHeartbeat() {
    const message = {
      type: 'HEARTBEAT',
      timestamp: Date.now(),
      connectedClients: this.clients.size,
      uptime: process.uptime() * 1000
    };

    for (const [socket] of this.clients.entries()) {
      this.sendToClient(socket, message);
    }
  }

  stopServer() {
    if (this.heartbeatInterval) {
      clearInterval(this.heartbeatInterval);
    }

    // Close all client connections
    for (const [socket] of this.clients.entries()) {
      socket.end();
    }

    if (this.server) {
      this.server.close(() => {
        console.log('[TCP API] Server closed');
      });
    }
  }
}

// Initialize timing module instances
const pairManager = new ExtensionPairManager();
const latencyTracker = new LatencyTracker();
const audioBufferManager = new AudioBufferManager();
const dashboardTCPAPI = new DashboardTCPAPI();

console.log('[Server] ‚úì AudioBufferManager initialized (ready for Step 4)');

// Auto-pair 9007 and 9008 on startup
pairManager.registerPair('9007', '9008');

// Start TCP API server
dashboardTCPAPI.startServer(6211);

// STEP 3: Extension buffer settings storage
// Store per-extension buffer settings from dashboard
const extensionBufferSettings = new Map();

// Initialize with defaults (autoSync: true per user request)
extensionBufferSettings.set('9007', { autoSync: true, manualLatencyMs: 0 });
extensionBufferSettings.set('9008', { autoSync: true, manualLatencyMs: 0 });

console.log('[TimingModule] Step 1: Classes initialized (not yet integrated into pipeline)');
console.log('[TimingModule] Step 3: Buffer settings storage initialized (autoSync: ON by default)');
// ============================================================================
// END OF TIMING & BUFFERING MODULE CLASSES
// ============================================================================

// Store active rooms and participants
const rooms = new Map();
const participants = new Map();

// Store user profiles for HMLCP
const userProfiles = new Map(); // key: userId_language, value: { profile, uloLayer }

// QA Settings: Per-extension language configuration
// Extension 9007: English ‚Üí French (DEFAULT)
// Extension 7888: French ‚Üí English (OPPOSITE - for bidirectional translation)
global.qaConfigs = new Map();
global.qaConfigs.set('9007', { sourceLang: 'en', targetLang: 'fr', qaMode: false });
global.qaConfigs.set('7888', { sourceLang: 'fr', targetLang: 'en', qaMode: false });

// Helper function to get config for extension (with fallback)
function getQaConfig(extension) {
  return global.qaConfigs.get(extension) || global.qaConfigs.get('9007');
}

// Store streaming Deepgram connections per socket
const streamingConnections = new Map(); // key: socket.id, value: { connection, customVocab }
// Per-extension audio gain factors (key: extension, value: gainFactor)
const extensionGainFactors = new Map(); // Default 1.2x for all
const humeConnections = new Map(); // key: socket.id, value: HumeStreamingClient instance
const humeAudioBuffers = new Map(); // key: socket.id, value: array of audio chunks to buffer before sending to Hume
const socketToExtension = new Map(); // key: socket.id, value: extension (for Hume emotion events)
const humeLatencyPerExtension = new Map(); // key: extension, value: latest Hume latency in ms
const humeStartTimestamps = new Map(); // key: extension, value: timestamp when last audio chunk was sent to Hume
const HUME_BUFFER_SIZE = 50; // 50 chunks = ~1 second at 20ms per chunk

// Language mapping for services
const languageMap = {
  'en': { name: 'English', deepgram: 'en-US', deepl: 'en-US', azure: 'en-US' },
  'es': { name: 'Spanish', deepgram: 'es', deepl: 'ES', azure: 'es-ES' },
  'fr': { name: 'French', deepgram: 'fr', deepl: 'FR', azure: 'fr-FR' },
  'de': { name: 'German', deepgram: 'de', deepl: 'DE', azure: 'de-DE' },
  'it': { name: 'Italian', deepgram: 'it', deepl: 'IT', azure: 'it-IT' },
  'pt': { name: 'Portuguese', deepgram: 'pt', deepl: 'PT-PT', azure: 'pt-PT' },
  'ja': { name: 'Japanese', deepgram: 'ja', deepl: 'JA', azure: 'ja-JP' },
  'ko': { name: 'Korean', deepgram: 'ko', deepl: 'KO', azure: 'ko-KR' },
  'zh': { name: 'Chinese', deepgram: 'zh', deepl: 'ZH', azure: 'zh-CN' },
  'ru': { name: 'Russian', deepgram: 'ru', deepl: 'RU', azure: 'ru-RU' }
};

// Deepgram STT function with HMLCP custom vocabulary support
// Create WAV header for raw PCM audio so Deepgram can process it
function createWavHeader(pcmData) {
  const pcmLength = pcmData.length;
  const header = Buffer.alloc(44);
  header.write("RIFF", 0);
  header.writeUInt32LE(36 + pcmLength, 4);
  header.write("WAVE", 8);
  header.write("fmt ", 12);
  header.writeUInt32LE(16, 16);
  header.writeUInt16LE(1, 20);
  header.writeUInt16LE(1, 22);
  header.writeUInt32LE(16000, 24);
  header.writeUInt32LE(32000, 28);
  header.writeUInt16LE(2, 32);
  header.writeUInt16LE(16, 34);
  header.write("data", 36);
  header.writeUInt32LE(pcmLength, 40);
  return Buffer.concat([header, pcmData]);
}

// ========================================
// Audio Amplifier (fixes low volume issue)
// ========================================
function amplifyAudio(pcmBuffer, gainFactor = 1.0) {
  const amplified = Buffer.alloc(pcmBuffer.length);
  let maxSample = 0;
  let clippedSamples = 0;
  
  for (let i = 0; i < pcmBuffer.length; i += 2) {
    let sample = pcmBuffer.readInt16LE(i);
    maxSample = Math.max(maxSample, Math.abs(sample));
    
    let amplifiedSample = Math.round(sample * gainFactor);
    
    if (amplifiedSample > 32767) {
      amplifiedSample = 32767;
      clippedSamples++;
    } else if (amplifiedSample < -32768) {
      amplifiedSample = -32768;
      clippedSamples++;
    }
    
    amplified.writeInt16LE(amplifiedSample, i);
  }
  
  console.log(`[Audio Amplifier] Gain: ${gainFactor}x, Max input: ${maxSample}, Clipped: ${clippedSamples} samples (${(clippedSamples/(pcmBuffer.length/2)*100).toFixed(2)}%)`);
  return amplified;
}

async function transcribeAudio(audioBuffer, language, customVocab = []) {
  if (!deepgramApiKey) {
    console.warn('Deepgram API key not set');
    return { text: '[STT not configured]', confidence: 0 };
  }

  try {
    console.log(`[Deepgram] Starting transcription:`);
    console.log(`  - Audio buffer size: ${audioBuffer.length} bytes`);
    console.log(`  - Language: ${language}`);
    console.log(`  - Deepgram language code: ${languageMap[language]?.deepgram || 'en-US'}`);

    const deepgram = createClient(deepgramApiKey);

    // Build Deepgram options
    const options = {
      model: 'nova-2',
      language: languageMap[language]?.deepgram || 'en-US',
      smart_format: true,
      punctuate: true,
      utterances: false
    };

    // Add HMLCP custom vocabulary if provided
    if (customVocab && customVocab.length > 0) {
      // Deepgram keywords format: "phrase:boost"
      options.keywords = customVocab.map(v => `${v.phrase}:${v.boost}`);
      console.log(`[HMLCP] Using ${customVocab.length} custom vocabulary terms for STT`);
    }

    console.log(`[Deepgram] Options:`, JSON.stringify(options, null, 2));

    // Amplify audio before sending to Deepgram (fixes low volume issue)
    const amplifiedAudio = audioBuffer;
    const { result, error } = await deepgram.listen.prerecorded.transcribeFile(createWavHeader(amplifiedAudio), options);

    if (error) {
      console.error('[Deepgram] API returned error:');
      console.error('  Error object:', JSON.stringify(error, null, 2));
      return { text: '', confidence: 0 };
    }

    console.log('[Deepgram] Raw result structure:');
    console.log('  Has result:', !!result);
    console.log('  Has results:', !!result?.results);
    console.log('  Has channels:', !!result?.results?.channels);
    console.log('  Channels length:', result?.results?.channels?.length || 0);

    if (result?.results?.channels && result.results.channels.length > 0) {
      console.log('  Channel[0] alternatives:', result.results.channels[0]?.alternatives?.length || 0);

      if (result.results.channels[0]?.alternatives && result.results.channels[0].alternatives.length > 0) {
        const alt = result.results.channels[0].alternatives[0];
        console.log('  Transcript:', alt.transcript);
        console.log('  Confidence:', alt.confidence);
        console.log('  Words count:', alt.words?.length || 0);
      } else {
        console.log('[Deepgram] No alternatives in channel[0]');
      }
    } else {
      console.log('[Deepgram] No channels in result');
      console.log('[Deepgram] Full result:', JSON.stringify(result, null, 2));
    }

    const transcript = result?.results?.channels[0]?.alternatives[0]?.transcript || '';
    const confidence = result?.results?.channels[0]?.alternatives[0]?.confidence || 0;

    if (!transcript || transcript.trim() === '') {
      console.log('[Deepgram] Empty transcription returned');
    } else {
      console.log(`[Deepgram] SUCCESS: "${transcript}" (confidence: ${confidence})`);
    }

    return { text: transcript, confidence };
  } catch (error) {
    console.error('[Deepgram] Exception during transcription:');
    console.error('  Error message:', error.message);
    console.error('  Error stack:', error.stack);
    console.error('  Error object:', JSON.stringify(error, Object.getOwnPropertyNames(error), 2));
    return { text: '', confidence: 0 };
  }
}


// DeepL translation function
async function translateText(text, sourceLang, targetLang) {
  if (!translator) {
    console.warn('DeepL not configured');
    return `[Translation: ${text}]`;
  }

  if (!text || text.trim() === '') {
    return text;
  }

  // QA Mode: Override languages with qaConfig if QA mode is enabled
  if (global.qaConfig && (global.qaConfig.sourceLang || global.qaConfig.targetLang)) {
    const originalSource = sourceLang;
    const originalTarget = targetLang;
    
    // Override with QA config languages
    sourceLang = global.qaConfig.sourceLang || sourceLang;
    targetLang = global.qaConfig.targetLang || targetLang;
    
    console.log(`[QA Config] Language override: ${originalSource} ‚Üí ${originalTarget} becomes ${sourceLang} ‚Üí ${targetLang}`);
  }

  // Skip translation if source === target (applies to both QA mode and normal mode)
  if (sourceLang === targetLang) {
    console.log(`[Translation] Bypassed: ${sourceLang} ‚Üí ${targetLang} (same language)`);
    return text;
  }

  try {
    const sourceCode = languageMap[sourceLang]?.deepl || 'en-US';
    const targetCode = languageMap[targetLang]?.deepl || 'en-US';

    console.log(`[Translation] ${sourceLang} ‚Üí ${targetLang}: "${text.substring(0, 50)}..."`);

    const result = await translator.translateText(
      text,
      sourceCode === 'en-US' ? null : sourceCode,
      targetCode
    );

    return result.text;
  } catch (error) {
    console.error('Translation error:', error);
    return text;
  }
}
async function synthesizeSpeech(text, language) {
  if (!elevenlabsTTS) {
    console.warn('ElevenLabs TTS not configured');
    return null;
  }

  try {
    // Use the default voice ID from .env
    const voiceId = elevenlabsVoiceId || 'XPwQNE5RX9Rdhyx0DWcI'; // Boyan Tiholov

    // Synthesize using ElevenLabs (returns MP3 buffer)
    const result = await elevenlabsTTS.synthesize(text, voiceId, {
      modelId: 'eleven_multilingual_v2' // Supports 29 languages
    });

    if (result && result.audio) {
      return result.audio; // Return the audio buffer
    }

    return null;
  } catch (error) {
    console.error('ElevenLabs TTS error:', error);
    return null;
  }
}

// HMLCP: Get or create user profile with ULO layer
async function getUserProfile(userId, language) {
  const key = `${userId}_${language}`;

  if (!userProfiles.has(key)) {
    try {
      // Try to load existing profile or create new one
      const profile = await UserProfile.load(userId, language);
      const uloLayer = new ULOLayer(profile);
      const patternExtractor = new PatternExtractor();

      userProfiles.set(key, { profile, uloLayer, patternExtractor });
      console.log(`[HMLCP] Loaded profile for ${userId} (${language})`);
    } catch (error) {
      console.error(`[HMLCP] Error loading profile for ${userId}:`, error);
      // Create new profile on error
      const profile = new UserProfile(userId, language);
      const uloLayer = new ULOLayer(profile);
      const patternExtractor = new PatternExtractor();
      userProfiles.set(key, { profile, uloLayer, patternExtractor });
    }
  }

  return userProfiles.get(key);
}

// Create streaming Deepgram connection for real-time STT
async function createStreamingConnection(socket, participant) {
  if (!deepgramApiKey) {
    console.warn('[Streaming STT] Deepgram API key not set');
    return null;
  }

  // Get user profile and custom vocabulary for HMLCP BEFORE creating connection
  const { profile, uloLayer, patternExtractor } = await getUserProfile(
    participant.username,
    participant.language
  );
  const customVocab = uloLayer.generateCustomVocabulary();

  const deepgram = createClient(deepgramApiKey);

  // Configure streaming options with HMLCP custom vocabulary
  const options = {
    model: 'nova-2',
    language: languageMap[participant.language]?.deepgram || 'en-US',
    smart_format: true,
    punctuate: true,
    interim_results: false,  // Only get final results for accuracy
    endpointing: 300,  // 300ms silence to finalize utterance (faster than 800ms)
    utterance_end_ms: 1000  // Max 1 second to close utterance
  };

  // Add HMLCP custom vocabulary if available
  if (customVocab && customVocab.length > 0) {
    options.keywords = customVocab.map(v => `${v.phrase}:${v.boost}`);
    console.log(`[Streaming STT] ${participant.username}: Using ${customVocab.length} custom vocabulary terms`);
  }

  // Create live streaming connection and attach error handler IMMEDIATELY
  let connection;
  try {
    connection = deepgram.listen.live(options);

    // CRITICAL: Attach error handler IMMEDIATELY in same tick to catch early errors
    connection.on('Error', (error) => {
      console.error(`[Streaming STT] ${participant.username}: Connection error:`, error.message);
      streamingConnections.delete(socket.id);

      // Clean up Hume AI connection
      const humeClient = humeConnections.get(socket.id);
      if (humeClient) {
        humeClient.disconnect();
        humeConnections.delete(socket.id);
      }
      humeAudioBuffers.delete(socket.id);
      socketToExtension.delete(socket.id);
      socket.emit('pipeline-log', {
        type: 'client',
        stage: 'error',
        message: `Streaming STT error - using batch mode`,
        timestamp: Date.now()
      });
    });

    // Increase max listeners to avoid warnings
    connection.setMaxListeners(20);

  } catch (error) {
    console.error(`[Streaming STT] ${participant.username}: Failed to create connection:`, error.message);
    return null;
  }

  // Handle connection close
  connection.on('Close', () => {
    console.log(`[Streaming STT] ${participant.username}: Connection closed`);
    streamingConnections.delete(socket.id);

      // Clean up Hume AI connection
      const humeClient = humeConnections.get(socket.id);
      if (humeClient) {
        humeClient.disconnect();
        humeConnections.delete(socket.id);
      }
      humeAudioBuffers.delete(socket.id);
      socketToExtension.delete(socket.id);
  });

  // Handle transcription results
  connection.on('Results', async (data) => {
    const transcript = data.channel?.alternatives[0]?.transcript;

    if (!transcript || transcript.trim() === '') {
      return;  // Skip empty transcriptions
    }

    const confidence = data.channel?.alternatives[0]?.confidence || 0;
    const isFinal = data.is_final;

    // Only process final results
    if (isFinal) {
      const startTime = Date.now();
      console.log(`[Streaming STT] ${participant.username}: "${transcript}" (confidence: ${(confidence * 100).toFixed(1)}%)`);

      // Log STT complete
      socket.emit('pipeline-log', {
        type: 'client',
        stage: 'stt-complete',
        message: `Speech recognized: "${transcript}"`,
        timestamp: Date.now()
      });

      // HMLCP: Apply ULO layer for personalized processing
      const processedTranscription = uloLayer.apply(transcript);

      // Store sample for learning
      profile.addTextSample(transcript);

      // Log ULO processing if modified
      if (processedTranscription !== transcript) {
        console.log(`[HMLCP] ${participant.username}: ULO applied: "${transcript}" ‚Üí "${processedTranscription}"`);
        socket.emit('pipeline-log', {
          type: 'client',
          stage: 'hmlcp-ulo',
          message: `Personalized processing applied`,
          timestamp: Date.now()
        });
      }

      const finalTranscription = processedTranscription;

      // Send transcription to speaker
      socket.emit('transcription-result', {
        text: finalTranscription,
        rawText: transcript,
        confidence,
        language: participant.language
      });

      // Get room participants for translation
      const room = rooms.get(participant.roomId);
      if (!room) return;

      // Translate and synthesize for each other participant
      const translationPromises = Array.from(room.participants)
        .map(participantId => participants.get(participantId))
        .filter(p => {
          if (!p || p.id === socket.id) return false;  // Skip speaker
          // ECHO PREVENTION: Skip if target has same language (no translation needed)
          if (p.language === participant.language) {
            console.log(`[Echo Prevention] Skipping ${p.username} - same language as ${participant.username} (${participant.language})`);
            return false;
          }
          return true;
        })
        .map(async (targetParticipant) => {
          try {
            const transStart = Date.now();

            // Translate
            const translatedText = await translateText(
              finalTranscription,
              participant.language,
              targetParticipant.language
            );

            const transEnd = Date.now();
            const transDuration = transEnd - transStart;

            // TTS
            const ttsStart = Date.now();
            const audioData = await synthesizeSpeech(
              translatedText,
              targetParticipant.language
            );

            const ttsEnd = Date.now();
            const ttsDuration = ttsEnd - ttsStart;
            const totalLatency = Date.now() - startTime;

            // Calculate STT time as the remainder (STT happened before startTime was set)
            // In streaming mode, STT time is from audio send to transcript receipt
            // We approximate it as total - (translation + tts)
            const sttDuration = totalLatency - transDuration - ttsDuration;

            console.log(`[Streaming] ${participant.username} ‚Üí ${targetParticipant.username}: ${totalLatency}ms total (STT: ${sttDuration}ms, Trans: ${transDuration}ms, TTS: ${ttsDuration}ms)`);

            // Send to target participant
            io.to(targetParticipant.id).emit('translated-audio', {
              originalText: finalTranscription,
              rawTranscription: transcript,
              translatedText,
              audioData: audioData ? audioData.toString('base64') : null,
              speakerUsername: participant.username,
              speakerLanguage: participant.language,
              latency: totalLatency,
              timing: {
                stt: sttDuration,
                translation: transDuration,
                tts: ttsDuration,
                total: totalLatency
              }
            });

            io.to(targetParticipant.id).emit('pipeline-log', {
              type: 'client',
              stage: 'complete',
              message: `‚úì Complete: ${totalLatency}ms (Trans: ${transDuration}ms, TTS: ${ttsDuration}ms)`,
              timestamp: Date.now()
            });

          } catch (error) {
            console.error(`[Streaming] ${participant.username} ‚Üí ${targetParticipant.username}: Error:`, error);
          }
        });

      await Promise.all(translationPromises);

      socket.emit('pipeline-log', {
        type: 'client',
        stage: 'speaker-confirmed',
        message: `Your speech sent to ${translationPromises.length} participant(s)`,
        timestamp: Date.now()
      });
    }
  });

  // Open the connection
  connection.on('Open', () => {
    console.log(`[Streaming STT] ${participant.username}: Connection opened successfully`);
    socket.emit('pipeline-log', {
      type: 'client',
      stage: 'info',
      message: `‚úì Streaming mode active (low-latency)`,
      timestamp: Date.now()
    });
  });

  // Store connection
  streamingConnections.set(socket.id, {

    connection,
    customVocab,
    profile,
    uloLayer,
    patternExtractor
  });

  // Initialize Hume AI client for emotion detection
  initializeHumeClient(socket.id).catch(err => console.error("[Hume] Init error:", err));
  console.log(`[Streaming STT] ${participant.username}: Streaming connection created`);
  return connection;
}

// Socket.io connection handling
// Initialize Hume AI client for a socket
async function initializeHumeClient(socketId) {
  if (!humeApiKey) {
    console.log('[Hume] API key not configured');
    return null;
  }
  
  try {
    const humeClient = new HumeStreamingClient(humeApiKey, {
      sampleRate: 16000,
      channels: 1
    });
    
    await humeClient.connect();
    
    // Handle metrics from Hume
    console.log('[DEBUG] Attaching metrics listener for socket:', socketId);
    humeClient.on('metrics', (metrics) => {
      console.log('[DEBUG] üìä Metrics event received from Hume:', {
        arousal: metrics.arousal,
        valence: metrics.valence,
        energy: metrics.energy,
        socketId: socketId
      });

      // Get extension for this socket
      const extension = socketToExtension.get(socketId);

      if (extension) {
        // Calculate Hume latency (time from audio sent to Hume ‚Üí metrics received)
        const startTime = humeStartTimestamps.get(extension);
        if (startTime) {
          const humeLatency = Date.now() - startTime;
          humeLatencyPerExtension.set(extension, humeLatency);
          console.log(`[Hume] Latency for extension ${extension}: ${humeLatency}ms`);
        }

        console.log('[DEBUG] ‚úÖ Broadcasting emotion_detected to all clients for extension:', extension);
        // Broadcast to ALL clients (including dashboard) - similar to how TTS works
        global.io.emit('emotion_detected', {
          extension: String(extension), // Add extension field for filtering
          arousal: metrics.arousal,
          valence: metrics.valence,
          energy: metrics.energy,
          timestamp: metrics.timestamp
        });
        console.log('[DEBUG] Emotion event broadcasted successfully');
      } else {
        console.error('[DEBUG] ‚ùå Extension not found for socketId:', socketId);
      }
    });
    
    humeConnections.set(socketId, humeClient);
    humeAudioBuffers.set(socketId, []); // Initialize empty buffer for this socket
    console.log(`[Hume] Client initialized for socket ${socketId}`);
    return humeClient;
  } catch (error) {
    console.error('[Hume] Error initializing client:', error.message);
    return null;
  }
}

// ========================================
// PHASE 2D: Gateway Audio Buffer Tracking
// Track audio buffers per extension for AI pipeline processing
// ========================================

// Track audio buffers per extension
const gatewayAudioBuffers = new Map(); // key: extension, value: { chunks: [], totalBytes: 0 }

// Configuration
const BUFFER_SIZE_THRESHOLD = 64000; // ~1 second at 16kHz mono PCM (2 bytes per sample)
/**
 * Add WAV header to raw PCM data
 * Format: 16kHz, mono, 16-bit PCM
 */
function addWavHeader(pcmBuffer) {
  const sampleRate = 16000;
  const numChannels = 1;
  const bitsPerSample = 16;
  const byteRate = sampleRate * numChannels * (bitsPerSample / 8);
  const blockAlign = numChannels * (bitsPerSample / 8);
  
  const wavHeader = Buffer.alloc(44);
  
  // RIFF chunk descriptor
  wavHeader.write('RIFF', 0);
  wavHeader.writeUInt32LE(36 + pcmBuffer.length, 4);
  wavHeader.write('WAVE', 8);
  
  // fmt sub-chunk
  wavHeader.write('fmt ', 12);
  wavHeader.writeUInt32LE(16, 16); // Subchunk1Size (16 for PCM)
  wavHeader.writeUInt16LE(1, 20); // AudioFormat (1 for PCM)
  wavHeader.writeUInt16LE(numChannels, 22);
  wavHeader.writeUInt32LE(sampleRate, 24);
  wavHeader.writeUInt32LE(byteRate, 28);
  wavHeader.writeUInt16LE(blockAlign, 32);
  wavHeader.writeUInt16LE(bitsPerSample, 34);
  
  // data sub-chunk
  wavHeader.write('data', 36);
  wavHeader.writeUInt32LE(pcmBuffer.length, 40);
  
  return Buffer.concat([wavHeader, pcmBuffer]);
}

/**
 * Process Gateway audio through AI pipeline
 * Called when buffer reaches ~1 second threshold
 */
async function processGatewayAudio(socket, extension, audioBuffer, language) {
  try {
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // TIMING INITIALIZATION - STEP 2: Timing Collection (Measure Only)
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    const timing = {
      extension,
      t0_gatewayReceived: Date.now(),
      stages: {},
      parallel: {}
    };

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 1: Gateway ‚Üí ASR
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    timing.t1_asrStarted = Date.now();
    timing.stages.audiosocket_to_asr = timing.t1_asrStarted - timing.t0_gatewayReceived;

    latencyTracker.updateStageLatency(extension, 'audiosocket_to_asr', timing.stages.audiosocket_to_asr);
    console.log('[Timing] Stage 1 (Gateway‚ÜíASR) for ' + extension + ': ' + timing.stages.audiosocket_to_asr + 'ms');

    // Step 1: Transcribe (ASR)
    console.log(`[Pipeline] Transcribing ${audioBuffer.length} bytes from extension ${extension}...`);
    // Amplify audio to improve Deepgram transcription accuracy
    // Use per-extension gain factor if set, otherwise default to 1.2
    const gainFactor = extensionGainFactors.get(extension) || 1.2;
    const amplifiedAudio = amplifyAudio(audioBuffer, gainFactor);
    // Add WAV header to raw PCM data for Deepgram
    const wavAudio = addWavHeader(amplifiedAudio);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 2: ASR Processing (Deepgram)
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    const asrStart = Date.now();
    const { text: transcription, confidence } = await transcribeAudio(wavAudio, language);
    timing.t2_asrCompleted = Date.now();
    timing.stages.asr = timing.t2_asrCompleted - asrStart;

    latencyTracker.updateStageLatency(extension, 'asr', timing.stages.asr);
    console.log('[Timing] Stage 2 (ASR) for ' + extension + ': ' + timing.stages.asr + 'ms');

    if (!transcription || transcription.trim() === '') {
      console.log(`[Pipeline] No transcription for extension ${extension}`);
      return;
    }

    console.log(`[Pipeline] Transcription from ${extension}: "${transcription}" (confidence: ${confidence})`);

    // Emit transcription to dashboard
    global.io.emit('transcriptionFinal', {
      extension: extension,
      text: transcription,
      language: language,
      confidence: confidence,
      timestamp: Date.now()
    });

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 3: ASR ‚Üí MT
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    timing.t3_mtStarted = Date.now();
    timing.stages.asr_to_mt = timing.t3_mtStarted - timing.t2_asrCompleted;

    latencyTracker.updateStageLatency(extension, 'asr_to_mt', timing.stages.asr_to_mt);
    console.log('[Timing] Stage 3 (ASR‚ÜíMT) for ' + extension + ': ' + timing.stages.asr_to_mt + 'ms');

    // Step 2: Translate
    const targetLang = extension === '9007' ? 'fr' : 'en'; // 9007 is English->French, 9008 is French->English
    console.log(`[Pipeline] Translating ${language} -> ${targetLang}: "${transcription}"`);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 4: MT Processing (DeepL)
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    const mtStart = Date.now();
    const translation = await translateText(transcription, language, targetLang);
    timing.t4_mtCompleted = Date.now();
    timing.stages.mt = timing.t4_mtCompleted - mtStart;

    latencyTracker.updateStageLatency(extension, 'mt', timing.stages.mt);
    console.log('[Timing] Stage 4 (MT) for ' + extension + ': ' + timing.stages.mt + 'ms');

    console.log(`[Pipeline] Translation: "${translation}" (${timing.stages.mt}ms)`);

    // Emit translation to dashboard
    global.io.emit('translationComplete', {
      extension: extension,
      original: transcription,
      translation: translation,
      sourceLang: language,
      targetLang: targetLang,
      timestamp: Date.now()
    });

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 5: MT ‚Üí TTS
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    timing.t5_ttsStarted = Date.now();
    timing.stages.mt_to_tts = timing.t5_ttsStarted - timing.t4_mtCompleted;

    latencyTracker.updateStageLatency(extension, 'mt_to_tts', timing.stages.mt_to_tts);
    console.log('[Timing] Stage 5 (MT‚ÜíTTS) for ' + extension + ': ' + timing.stages.mt_to_tts + 'ms');

    // Step 3: TTS - Generate audio from translation
    console.log(`[Pipeline] Generating TTS for extension ${extension}: "${translation}"`);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 6: TTS Processing (ElevenLabs)
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    const ttsStart = Date.now();
    const ttsAudio = await synthesizeSpeech(translation, targetLang);
    timing.t6_ttsCompleted = Date.now();
    timing.stages.tts = timing.t6_ttsCompleted - ttsStart;

    latencyTracker.updateStageLatency(extension, 'tts', timing.stages.tts);
    console.log('[Timing] Stage 6 (TTS) for ' + extension + ': ' + timing.stages.tts + 'ms - ' + (ttsAudio ? ttsAudio.length : 0) + ' bytes');

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 7: TTS ‚Üí LS (Latency Sync preparation)
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    timing.t7_lsStarted = Date.now();
    timing.stages.tts_to_ls = timing.t7_lsStarted - timing.t6_ttsCompleted;

    latencyTracker.updateStageLatency(extension, 'tts_to_ls', timing.stages.tts_to_ls);
    console.log('[Timing] Stage 7 (TTS‚ÜíLS) for ' + extension + ': ' + timing.stages.tts_to_ls + 'ms');

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 8: LS (Latency Sync - Buffer Calculation)
    // STEP 4.1 & 4.2: Retrieve settings and calculate buffer delay
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    const pairedExtension = pairManager.getPairedExtension(extension);

    // SUB-TASK 4.1: Retrieve Buffer Settings
    const settings = extensionBufferSettings.get(extension) || { autoSync: false, manualLatencyMs: 0 };
    const autoSync = settings.autoSync;
    const manualLatencyMs = settings.manualLatencyMs;

    console.log(`[Buffer Apply] Extension ${extension} settings: autoSync=${autoSync}, manualLatency=${manualLatencyMs}ms`);

    // SUB-TASK 4.2: Calculate Total Buffer Delay
    const latencyDifference = latencyTracker.getCurrentLatencyDifference(extension, pairedExtension);
    let totalBufferMs = manualLatencyMs; // Always add manual adjustment

    if (autoSync && latencyDifference !== null && latencyDifference < 0) {
      // This extension is FASTER (negative latency diff) - needs buffer to sync
      const autoSyncBufferMs = Math.abs(latencyDifference);
      totalBufferMs += autoSyncBufferMs;
      console.log(`[Buffer Apply] Extension ${extension} is FASTER by ${Math.round(autoSyncBufferMs)}ms`);
      console.log(`[Buffer Apply] Auto-sync buffer: +${Math.round(autoSyncBufferMs)}ms`);
    } else if (autoSync && latencyDifference !== null && latencyDifference > 0) {
      // This extension is SLOWER (positive latency diff) - no auto-sync buffer needed
      console.log(`[Buffer Apply] Extension ${extension} is SLOWER by ${Math.round(latencyDifference)}ms - no auto-sync buffer needed`);
    } else if (latencyDifference === null) {
      console.log(`[Buffer Apply] No latency data yet for synchronization`);
    }

    console.log(`[Buffer Apply] Total buffer: ${Math.round(totalBufferMs)}ms (manual: ${manualLatencyMs}ms, auto: ${Math.round(totalBufferMs - manualLatencyMs)}ms)`);

    // For now, don't actually apply the buffer yet (will be done in Step 4.4)
    const bufferDelayMs = 0; // Step 4.4 will replace this with actual buffer application

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STEP 4.3: CONVERT MP3 TO PCM16
    // Convert ElevenLabs MP3 audio to PCM16 format for Gateway/Asterisk
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    let pcmAudioBuffer = null;
    if (ttsAudio) {
      try {
        console.log(`[Audio Convert] Converting ${ttsAudio.length} bytes MP3 to PCM16...`);
        pcmAudioBuffer = await convertMp3ToPcm16(ttsAudio);
        console.log(`[Audio Convert] ‚úì Conversion complete: ${pcmAudioBuffer.length} bytes PCM16`);
      } catch (error) {
        console.error(`[Audio Convert] ‚úó Conversion failed for extension ${extension}:`, error.message);
        pcmAudioBuffer = null;
      }
    } else {
      console.log(`[Audio Convert] No TTS audio to convert for extension ${extension}`);
    }



    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STEP 4.4 & 4.5: APPLY BUFFER AND SEND TO GATEWAY
    // Apply calculated buffer delay and emit audio back to Gateway
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if (pcmAudioBuffer && pcmAudioBuffer.length > 0) {
      console.log(`[Buffer Send] Applying ${Math.round(totalBufferMs)}ms buffer to ${pcmAudioBuffer.length} bytes PCM16 for extension ${pairedExtension}`);
      
      audioBufferManager.bufferAndSend(
        pairedExtension,           // Target extension (paired partner)
        pcmAudioBuffer,            // PCM16 audio data
        totalBufferMs,             // Buffer delay in milliseconds
        (targetExtension, delayedAudio) => {
          // Callback executed after buffer delay
          console.log(`[Buffer Send] ‚úì Sending ${delayedAudio.length} bytes PCM16 to Gateway for extension ${targetExtension}`);
          
          // SUB-TASK 4.5: Emit translatedAudio (camelCase) to Gateway
          global.io.emit('translatedAudio', {
            extension: String(targetExtension),
            audio: delayedAudio,         // Buffer object (PCM16 data)
            format: 'pcm16',             // Audio format
            sampleRate: 16000,           // Sample rate
            channels: 1,                 // Mono
            timestamp: Date.now(),
            bufferApplied: totalBufferMs,
            sourceExtension: extension   // Original extension that generated this translation
          });
          
          console.log(`[Buffer Send] ‚úì translatedAudio emitted to Gateway for extension ${targetExtension} (buffered ${Math.round(totalBufferMs)}ms)`);
        }
      );
    } else {
      console.log(`[Buffer Send] ‚ö† No PCM audio available to send for extension ${extension}`);
    }

    timing.t8_lsCompleted = Date.now();
    timing.stages.ls = timing.t8_lsCompleted - timing.t7_lsStarted;

    latencyTracker.updateStageLatency(extension, 'ls', timing.stages.ls);
    console.log('[Timing] Stage 8 (LS) for ' + extension + ': ' + timing.stages.ls + 'ms');

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STAGE 9: LS ‚Üí Bridge (Send to gateway)
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // Emit TTS audio to dashboard
    console.log(`[TTS DEBUG] Emitting translated-audio for extension: ${extension} (type: ${typeof extension})`);
    global.io.emit('translated-audio', {
      extension: String(extension),
      original: transcription,
      translation: translation,
      audio: ttsAudio ? ttsAudio.toString('base64') : null,
      audioFormat: 'mp3',  // ElevenLabs returns MP3
      sourceLang: language,
      targetLang: targetLang,
      timestamp: Date.now(),
      timing: {
        tts: timing.stages.tts
      }
    });

    timing.t9_bridgeSent = Date.now();
    timing.stages.ls_to_bridge = timing.t9_bridgeSent - timing.t8_lsCompleted;

    latencyTracker.updateStageLatency(extension, 'ls_to_bridge', timing.stages.ls_to_bridge);
    console.log('[Timing] Stage 9 (LS‚ÜíBridge) for ' + extension + ': ' + timing.stages.ls_to_bridge + 'ms');

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // CALCULATE TOTAL E2E LATENCY
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    timing.serialTotal = timing.stages.audiosocket_to_asr +
                        timing.stages.asr +
                        timing.stages.asr_to_mt +
                        timing.stages.mt +
                        timing.stages.mt_to_tts +
                        timing.stages.tts +
                        timing.stages.tts_to_ls +
                        timing.stages.ls +
                        timing.stages.ls_to_bridge;

    // Get Hume parallel pipeline latency (if available)
    const humeLatency = humeLatencyPerExtension.get(extension) || 0;
    timing.parallelTotal = humeLatency;
    timing.e2eTotal = Math.max(timing.serialTotal, timing.parallelTotal);

    if (humeLatency > 0) {
      console.log(`[Timing] Hume parallel pipeline latency for ${extension}: ${humeLatency}ms`);
    }

    console.log('[Timing] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
    console.log('[Timing] Extension ' + extension + ' E2E Total: ' + Math.round(timing.e2eTotal) + 'ms');
    console.log('[Timing]   - Serial: ' + Math.round(timing.serialTotal) + 'ms');
    console.log('[Timing]   - Parallel: ' + Math.round(timing.parallelTotal) + 'ms');
    console.log('[Timing] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');

    // Update latency tracker with E2E total
    const direction = extension + '‚Üí' + pairedExtension;
    latencyTracker.updateLatency(direction, timing.e2eTotal);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // OLD STEP 2 CODE - REPLACED BY STEP 4.1 & 4.2 ABOVE
    // Buffer calculation now happens in STAGE 8 with dashboard settings
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // Use totalBufferMs calculated above in STAGE 8
    const targetBufferMs = totalBufferMs; // For dashboard compatibility

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // STEP 3: EMIT DASHBOARD UPDATES
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // Determine if parallel path is blocking (for now, always false since Hume not integrated)
    const isParallelBlocking = false;

    incrementUtteranceCount(extension);

    // Emit complete latency update to dashboard
    emitLatencyUpdate(extension, timing, bufferDelayMs, targetBufferMs, latencyDifference, isParallelBlocking);

    // STEP 2 FIX: Also emit inverted latency difference to paired extension for consistency
    // This ensures both dashboards always show mathematically consistent mirror values
    if (latencyDifference !== null) {
      emitLatencyUpdateToPairedExtension(pairedExtension, -latencyDifference);
    }

    // Emit individual stage timings via TCP API AND Socket.IO
    const stages = [
      { name: 'audiosocket_to_asr', duration: timing.stages.audiosocket_to_asr, label: 'Gateway ‚Üí ASR' },
      { name: 'asr', duration: timing.stages.asr, label: 'ASR (Deepgram)' },
      { name: 'asr_to_mt', duration: timing.stages.asr_to_mt, label: 'ASR ‚Üí MT' },
      { name: 'mt', duration: timing.stages.mt, label: 'MT (DeepL)' },
      { name: 'mt_to_tts', duration: timing.stages.mt_to_tts, label: 'MT ‚Üí TTS' },
      { name: 'tts', duration: timing.stages.tts, label: 'TTS (ElevenLabs)' },
      { name: 'tts_to_ls', duration: timing.stages.tts_to_ls, label: 'TTS ‚Üí LS' },
      { name: 'ls', duration: timing.stages.ls, label: 'LS (Latency Sync)' },
      { name: 'ls_to_bridge', duration: timing.stages.ls_to_bridge, label: 'LS ‚Üí Bridge' }
    ];

    stages.forEach(stage => {
      dashboardTCPAPI.broadcastStage(extension, stage.name, stage.duration, stage.label);
      io.emit('stage_timing', {
        type: 'STAGE_TIMING',
        extension,
        stage: stage.name,
        duration: stage.duration,
        stageName: stage.label,
        timestamp: Date.now()
      });
    });

    // Broadcast buffer calculation (STEP 4) - shows target buffer even if not applied yet
    if (targetBufferMs > 0) {
      dashboardTCPAPI.broadcastBufferCalculation(extension, targetBufferMs, pairedExtension, latencyDifference);
    }

    // Note: Buffer not applied yet in Step 4, so bufferDelayMs = 0
    if (bufferDelayMs > 0) {
      dashboardTCPAPI.broadcastBufferApplied(extension, bufferDelayMs, 'sync_to_' + pairedExtension, latencyDifference);
    }

  } catch (error) {
    console.error(`[Pipeline] Error in processGatewayAudio for ${extension}:`, error);
  }
}

// ============================================================================
// STEP 3: Dashboard Event Emitters
// ============================================================================

// Utterance counter for statistics
const utteranceCounts = new Map(); // extension ‚Üí count

function getUtteranceCount(extension) {
  return utteranceCounts.get(extension) || 0;
}

function incrementUtteranceCount(extension) {
  const count = (utteranceCounts.get(extension) || 0) + 1;
  utteranceCounts.set(extension, count);
  return count;
}

// Dashboard latency update emitter
function emitLatencyUpdate(extension, timing, appliedBufferMs, targetBufferMs, latencyDifference, isParallelBlocking) {
  const pairedExtension = pairManager.getPairedExtension(extension);

  const data = {
    extension: extension,

    latencies: latencyTracker.getAllLatencies(extension),

    buffer: {
      current: appliedBufferMs,        // Currently applied buffer (0 until STEP 5)
      target: targetBufferMs,           // Calculated target buffer from STEP 4
      adjustment: latencyDifference !== null ? Math.round(latencyDifference) : 0,  // SIGNED latency difference for dashboard bar (positive=slower, negative=faster)
      reason: 'sync_to_' + pairedExtension
    },

    displaySerialTotal: timing.serialTotal,
    displayParallelTotal: timing.parallelTotal,
    displaySerialTotalWithSync: timing.serialTotal + appliedBufferMs,

    parallelBlocking: isParallelBlocking,

    stats: {
      utteranceCount: getUtteranceCount(extension),
      avgUtteranceLatency: Math.round(latencyTracker.getAverageLatency(extension + '‚Üí' + pairedExtension))
    }
  };

  // Add E2E latency values
  data.latencies.e2e = {
    current: timing.e2eTotal,
    avg: data.stats.avgUtteranceLatency
  };
  data.latencies.serial_total = {
    current: timing.serialTotal,
    avg: timing.serialTotal
  };
  data.latencies.parallel_total = {
    current: timing.parallelTotal,
    avg: timing.parallelTotal
  };

  // Emit via Socket.IO to dashboard
  global.io.emit('latencyUpdate', data);

  // Broadcast via TCP API
  dashboardTCPAPI.broadcastLatencyUpdate(data);

  console.log('[Dashboard] Emitted latency update for ' + extension + ' (E2E: ' + Math.round(timing.e2eTotal) + 'ms, Diff: ' + data.buffer.adjustment + 'ms)');
}

// Helper function to emit latency difference update to paired extension only
// This ensures both dashboards always show mathematically consistent mirror values
function emitLatencyUpdateToPairedExtension(pairedExtension, invertedLatencyDifference) {
  const data = {
    extension: pairedExtension,
    buffer: {
      adjustment: Math.round(invertedLatencyDifference)  // Inverted difference for paired extension
    }
  };

  // Emit via Socket.IO to dashboard
  global.io.emit('latencyUpdate', data);

  console.log('[Dashboard] Emitted paired latency update for ' + pairedExtension + ' (Diff: ' + data.buffer.adjustment + 'ms)');
}

// ============================================================================
// END OF STEP 3 FUNCTIONS
// ============================================================================


io.on('connection', (socket) => {
  console.log('Client connected:', socket.id);

  // ========================================
  // PHASE 2C: Gateway WebSocket Handlers
  // Handle events from ExternalMedia Gateway (extensions 9007/9008)
  // ========================================
  
  // Track Gateway connections by extension
  const gatewayConnections = socket.gatewayConnections || new Map();
  
  // Register extension from Gateway
  socket.on('registerExtension', (data) => {
    console.log(`[Gateway] Extension ${data.extension} registered (${data.language})`);

    // Store Gateway connection info
    gatewayConnections.set(data.extension, {
      extension: data.extension,
      language: data.language,
      format: data.format,
      sampleRate: data.sampleRate,
      socketId: socket.id
    });

    // Store reverse mapping: socketId -> extension (for Hume emotion events)
    socketToExtension.set(socket.id, data.extension);
    console.log(`[Gateway] Socket ${socket.id} mapped to extension ${data.extension}`);

    // Initialize Hume AI client for emotion detection
    initializeHumeClient(socket.id).catch(err => console.error("[Hume] Init error:", err));

    socket.emit('registrationConfirmed', {
      extension: data.extension,
      status: 'ready'
    });
  });
  
  // Receive audio stream from Gateway
  socket.on('audioStream', async (data) => {
    const { extension, audio, timestamp } = data;
    
    // Initialize buffer for this extension if needed
    if (!gatewayAudioBuffers.has(extension)) {
      gatewayAudioBuffers.set(extension, {
        chunks: [],
        totalBytes: 0,
        language: gatewayConnections.get(extension)?.language || 'en'
      });
    }
    
    const buffer = gatewayAudioBuffers.get(extension);
    const audioChunk = Buffer.isBuffer(audio) ? audio : Buffer.from(audio);
    console.log("[RMS DEBUG] audioChunk.length:", audioChunk.length);
    console.log("[RMS DEBUG] audioChunk.length:", audioChunk.length);

    // Fork audio to Hume AI for emotion detection (buffered for better speech detection)
    const humeClient = humeConnections.get(socket.id);
    // Broadcast audio to all dashboard clients for monitoring
    global.io.emit('audioStream', {
      extension: extension,
      buffer: audioChunk,
      sampleRate: data.sampleRate || 16000,
      channels: data.channels || 1,
      timestamp: timestamp || Date.now()
    });
    if (humeClient && humeClient.connected) {
      const humeBuffer = humeAudioBuffers.get(socket.id);
      if (humeBuffer) {
        humeBuffer.push(audioChunk);

        // Send to Hume only when buffer reaches threshold (50 chunks = ~1 second)
        if (humeBuffer.length >= HUME_BUFFER_SIZE) {
          const combinedBuffer = Buffer.concat(humeBuffer);

          // Record timestamp when audio is sent to Hume
          const extension = socketToExtension.get(socket.id);
          if (extension) {
            humeStartTimestamps.set(extension, Date.now());
          }

          humeClient.sendAudio(combinedBuffer);
          console.log(`[Hume] Sent ${HUME_BUFFER_SIZE} chunks (${combinedBuffer.length} bytes, ~1 second buffer) for extension ${extension}`);

          // Reset buffer for next batch
          humeAudioBuffers.set(socket.id, []);
        }
      }
    }

    // Calculate enhanced audio metrics (RMS, peak, clipping) for professional monitoring
    let rmsLevel = 0;
    let peakLevel = 0;
    let clippingCount = 0;

    if (audioChunk.length > 0) {
      let sumSquares = 0;
      let maxSample = 0;

      for (let i = 0; i < audioChunk.length; i += 2) {
        const sample = audioChunk.readInt16LE(i);
        const absSample = Math.abs(sample);

        // RMS calculation
        sumSquares += sample * sample;

        // Peak calculation
        if (absSample > maxSample) {
          maxSample = absSample;
        }

        // Clipping detection (samples at or near max int16 value)
        if (absSample >= 32767 || absSample >= 32500) {
          clippingCount++;
        }
      }

      const sampleCount = audioChunk.length / 2;
      const rms = Math.sqrt(sumSquares / sampleCount);
      console.log("[RMS DEBUG] sumSquares:", sumSquares, "sampleCount:", sampleCount, "rms:", rms, "rmsLevel:", Math.min(100, Math.round((rms / 32768) * 100)));
      console.log("[RMS DEBUG] sumSquares:", sumSquares, "sampleCount:", sampleCount, "rms:", rms, "rmsLevel:", Math.min(100, Math.round((rms / 32768) * 100)));

      // Normalize to 0-100 for RMS
      rmsLevel = Math.min(100, Math.round((rms / 32768) * 100));
      console.log("[RMS DEBUG] Final rmsLevel:", rmsLevel, "peakLevel:", peakLevel, "clippingCount:", clippingCount);

      // Normalize peak to 0-100
      peakLevel = Math.min(100, Math.round((maxSample / 32768) * 100));
    }

    // Emit monitoring event for Extension Monitor card with enhanced audio metrics
    global.io.emit('transcriptionPartial', {
      extension: extension,
      bytes: audioChunk.length,
      timestamp: timestamp || Date.now(),
      uuid: gatewayConnections.get(extension)?.socketId || socket.id,
      audioLevel: rmsLevel,
      peakLevel: peakLevel,
      clipping: clippingCount
    });
    // Add chunk to buffer
    buffer.chunks.push(audioChunk);
    buffer.totalBytes += audioChunk.length;
    
    // Log every 50th packet to avoid flooding
    if (Math.random() < 0.02) {
      console.log(`[Gateway] Extension ${extension}: buffered ${buffer.totalBytes} bytes (${buffer.chunks.length} chunks)`);
    }
    
    // Process when we have enough audio (~1 second)
    if (buffer.totalBytes >= BUFFER_SIZE_THRESHOLD) {
      // Concatenate all chunks
      const combinedAudio = Buffer.concat(buffer.chunks);
      
      console.log(`[Gateway] Processing ${combinedAudio.length} bytes from extension ${extension}`);
      
      // Reset buffer for next batch
      buffer.chunks = [];
      buffer.totalBytes = 0;
      
      // Process through AI pipeline (async, don't wait)
      processGatewayAudio(socket, extension, combinedAudio, buffer.language).catch(err => {
        console.error(`[Gateway] Error processing audio for ${extension}:`, err.message);
      });
    }
  });
  // Join or create room
  socket.on('join-room', (data) => {
    const { roomId, username, language } = data;

    socket.join(roomId);

    // Store participant info
    participants.set(socket.id, {
      id: socket.id,
      username,
      language,
      roomId
    });

    // Initialize room if it doesn't exist
    if (!rooms.has(roomId)) {
      rooms.set(roomId, {
        id: roomId,
        participants: new Set()
      });
    }

    const room = rooms.get(roomId);
    room.participants.add(socket.id);

    console.log(`${username} joined room ${roomId} with language ${language}`);

    // Notify others in the room
    socket.to(roomId).emit('participant-joined', {
      participantId: socket.id,
      username,
      language
    });

    // Send current participants to the new user
    const currentParticipants = Array.from(room.participants)
      .filter(id => id !== socket.id)
      .map(id => participants.get(id))
      .filter(p => p);

    socket.emit('room-joined', {
      roomId,
      participants: currentParticipants
    });

    // Streaming STT temporarily disabled - API key authentication issues in production context
    // The standalone test passes, but server crashes with "non-101 status code" error
    // This suggests the API key works in isolation but not in server context
    // Falling back to stable batch mode (~1500-2400ms latency)
    const participant = participants.get(socket.id);
    if (participant) {
      createStreamingConnection(socket, participant)
        .then(connection => {
          if (connection) {
            console.log(`[Mode] ‚úÖ Streaming mode enabled for ${participant.username}`);
          } else {
            console.log(`[Mode] ‚ö†Ô∏è Streaming failed, falling back to batch mode for ${participant.username}`);
            socket.emit('pipeline-log', {
              type: 'client',
              stage: 'info',
              message: `‚ö†Ô∏è Batch mode active (streaming unavailable)`,
              timestamp: Date.now()
            });
          }
        })
        .catch(error => {
          console.error(`[Mode] Error creating streaming connection:`, error);
          socket.emit('pipeline-log', {
            type: 'client',
            stage: 'info',
            message: `‚ö†Ô∏è Batch mode active (streaming error)`,
            timestamp: Date.now()
          });
        });
    }
    
  });

  // Handle audio stream - feed to Deepgram Live connection or fallback to batch
  socket.on('audio-stream', async (data) => {
    const { audioBuffer, roomId } = data;
    const streamData = streamingConnections.get(socket.id);
    const participant = participants.get(socket.id);

    if (!participant) return;

    // Try streaming mode first
    if (streamData && streamData.connection) {
      try {
        // Send audio chunk to Deepgram Live API
        streamData.connection.send(Buffer.from(audioBuffer));

        // Fork audio to Hume AI for emotion detection
        const humeClient = humeConnections.get(socket.id);
        if (humeClient && humeClient.connected) {
          // Record timestamp when audio is sent to Hume
          const extension = socketToExtension.get(socket.id);
          if (extension) {
            humeStartTimestamps.set(extension, Date.now());
          }

          humeClient.sendAudio(Buffer.from(audioBuffer));
        }
      } catch (error) {
        console.error(`[Streaming] Error sending audio:`, error);
      }
    } else {
      // Fallback to batch mode if streaming not available
      // This prevents system from breaking if Deepgram Live API isn't available
      console.log(`[Batch Mode] Processing audio for ${participant.username} (streaming unavailable)`);

      try {
        const { profile, uloLayer } = await getUserProfile(
          participant.username,
          participant.language
        );
        const customVocab = uloLayer.generateCustomVocabulary();

        const startTime = Date.now();
        const sttStart = Date.now();
        const { text: transcription, confidence } = await transcribeAudio(
          Buffer.from(audioBuffer),
          participant.language,
          customVocab
        );

        const sttEnd = Date.now();
        const sttDuration = sttEnd - sttStart;

        if (!transcription || transcription.trim() === '') {
          return;
        }

        const processedTranscription = uloLayer.apply(transcription);
        profile.addTextSample(transcription);

        socket.emit('transcription-result', {
          text: processedTranscription,
          rawText: transcription,
          confidence,
          language: participant.language
        });

        // Translate for other participants
        const room = rooms.get(roomId);
        if (!room) return;

        const translationPromises = Array.from(room.participants)
          .map(participantId => participants.get(participantId))
          .filter(p => p && p.id !== socket.id)  // Skip speaker only
          .map(async (targetParticipant) => {
            const transStart = Date.now();
            const translatedText = await translateText(
              processedTranscription,
              participant.language,
              targetParticipant.language
            );

            const transEnd = Date.now();
            const transDuration = transEnd - transStart;

            const ttsStart = Date.now();
            const audioData = await synthesizeSpeech(
              translatedText,
              targetParticipant.language
            );

            const ttsEnd = Date.now();
            const ttsDuration = ttsEnd - ttsStart;
            const totalLatency = Date.now() - startTime;

            io.to(targetParticipant.id).emit('translated-audio', {
              originalText: processedTranscription,
              rawTranscription: transcription,
              translatedText,
              audioData: audioData ? audioData.toString('base64') : null,
              speakerUsername: participant.username,
              speakerLanguage: participant.language,
              latency: totalLatency,
              timing: {
                stt: sttDuration,
                translation: transDuration,
                tts: ttsDuration,
                total: totalLatency
              }
            });
          });

        await Promise.all(translationPromises);
      } catch (error) {
        console.error(`[Batch Mode] Error:`, error);
      }
    }
  });

  // Handle voice profile creation (from onboarding)
  socket.on('create-voice-profile', async (data) => {
    const { profileId, language, phrasesCount, skipped } = data;

    console.log(`[Onboarding] Creating voice profile: ${profileId} (${language}), skipped: ${skipped}`);

    try {
      // Initialize HMLCP profile for this user
      const { profile } = await getUserProfile(profileId, language);

      // If user skipped calibration, apply default language profile
      if (skipped || phrasesCount === 0) {
        console.log(`[Onboarding] Applying default ${language} profile (calibration skipped)`);
        applyDefaultProfile(profile, language);
        await profile.save();
      }

      socket.emit('profile-created', {
        profileId,
        language,
        success: true,
        isDefault: skipped || phrasesCount === 0
      });

      console.log(`[Onboarding] Profile created: ${profileId}${skipped ? ' (default profile)' : ''}`);
    } catch (error) {
      console.error('[Onboarding] Error creating profile:', error);
      socket.emit('error', { message: 'Failed to create profile' });
    }
  });

  // Handle calibration audio (from onboarding)
  socket.on('calibration-audio', async (data) => {
    const { audioBuffer, phrase, phraseIndex, language, profileId } = data;

    console.log(`[Onboarding] Calibration audio received: phrase ${phraseIndex + 1}, ${audioBuffer.byteLength} bytes`);

    try {
      // Transcribe the calibration audio
      const { text: transcription, confidence } = await transcribeAudio(
        Buffer.from(audioBuffer),
        language
      );

      console.log(`[Onboarding] Calibration phrase ${phraseIndex + 1} transcribed: "${transcription}" (expected: "${phrase}")`);

      // If we have a profile, add this as a calibration sample
      if (profileId) {
        const { profile, patternExtractor } = await getUserProfile(profileId, language);

        // Add to profile as a calibration sample
        profile.addTextSample(transcription);
        profile.addCalibrationSample(phrase, transcription);

        // Extract patterns if mismatch detected
        if (transcription.toLowerCase() !== phrase.toLowerCase()) {
          const pattern = patternExtractor.extractCorrectionPattern(transcription, phrase);
          if (pattern) {
            profile.addPattern(pattern);
            console.log(`[Onboarding] Pattern learned from calibration: ${transcription} ‚Üí ${phrase}`);
          }
        }

        // Save profile after each calibration phrase
        await profile.save();
      }

      // Notify client that processing is complete
      socket.emit('calibration-processed', {
        phraseIndex,
        transcription,
        expectedPhrase: phrase,
        confidence,
        success: true
      });

    } catch (error) {
      console.error('[Onboarding] Error processing calibration audio:', error);
      socket.emit('error', { message: 'Failed to process calibration audio' });
    }
  });

  // Handle disconnect

  // QA Settings: Handle language configuration from dashboard
  socket.on('qa-language-config', (config) => {
    const { sourceLang, targetLang, qaMode, extension } = config;

    if (!extension) {
      console.warn('[QA Config] ‚ö†Ô∏è  No extension specified, ignoring update');
      return;
    }

    // Update per-extension QA configuration
    global.qaConfigs.set(extension, { sourceLang, targetLang, qaMode });

    console.log(`[QA Config] ‚úì Extension ${extension} updated: ${sourceLang} ‚Üí ${targetLang} (QA Mode: ${qaMode})`);

    // Broadcast to all connected clients (they will filter by extension)
    io.emit('qa-config-updated', {
      extension,
      sourceLang,
      targetLang,
      qaMode
    });
  });

  // Handle volume control requests from dashboard
  socket.on('setVolume', (data) => {
    const { extension, volumeTX } = data;
    console.log(`[Volume Control] Request to set extension ${extension} TX gain to ${volumeTX} dB`);

    // Convert dB to linear gain factor: gainFactor = 10^(dB/20)
    const gainFactor = Math.pow(10, volumeTX / 20);

    // Store the gain factor for this extension
    extensionGainFactors.set(extension, gainFactor);

    console.log(`[Volume Control] ‚úì Extension ${extension} PCM gain set to ${gainFactor.toFixed(3)}x (${volumeTX} dB)`);
  });

  // ========================================
  // STEP 1: Latency & Sync Dashboard Event Handlers (Logging Only)
  // ========================================

  // Handle Auto Sync toggle from dashboard
  socket.on('setAutoSync', (data) => {
    const { extension, enabled } = data;
    console.log(`[Latency Dashboard] setAutoSync received for extension ${extension}:`, enabled);

    // STEP 3: Store the setting
    const settings = extensionBufferSettings.get(extension) || { autoSync: false, manualLatencyMs: 0 };
    settings.autoSync = enabled;
    extensionBufferSettings.set(extension, settings);
    console.log(`[Buffer Settings] Extension ${extension} autoSync set to ${enabled}`);

    socket.emit('autoSyncConfirmed', {
      extension,
      enabled,
      status: 'acknowledged'
    });
  });

  // Handle Manual Latency adjustment from dashboard
  socket.on('setManualLatency', (data) => {
    const { extension, latencyMs } = data;
    console.log(`[Latency Dashboard] setManualLatency received for extension ${extension}:`, latencyMs, 'ms');

    // STEP 3: Store the setting
    const settings = extensionBufferSettings.get(extension) || { autoSync: false, manualLatencyMs: 0 };
    settings.manualLatencyMs = latencyMs;
    extensionBufferSettings.set(extension, settings);
    console.log(`[Buffer Settings] Extension ${extension} manualLatencyMs set to ${latencyMs}ms`);

    socket.emit('manualLatencyConfirmed', {
      extension,
      latencyMs,
      status: 'acknowledged'
    });
  });

  // Handle Audio Monitor request from dashboard
  socket.on('requestAudioMonitor', (data) => {
    const { extension } = data;
    console.log(`[Latency Dashboard] requestAudioMonitor received for extension ${extension}`);

    // Step 1: Just log, implementation will come in Step 5
    socket.emit('audioMonitorStatus', {
      extension,
      status: 'not_implemented',
      message: 'Audio monitoring will be implemented in Step 5'
    });
  });

  // ============================================================================
  // TEST MODE CONTROL - Socket.IO event handlers
  // ============================================================================

  // Start test mode - send looping test audio to target extension
  socket.on('startTestMode', (data) => {
    const { targetExtension } = data;
    console.log(`[Test Mode] Start request received for extension ${targetExtension}`);

    const result = startTestMode(targetExtension);

    // Broadcast status to all connected dashboards
    io.emit('testModeStatus', {
      ...result,
      ...getTestModeStatus()
    });
  });

  // Stop test mode
  socket.on('stopTestMode', () => {
    console.log(`[Test Mode] Stop request received`);

    const result = stopTestMode();

    // Broadcast status to all connected dashboards
    io.emit('testModeStatus', {
      ...result,
      ...getTestModeStatus()
    });
  });

  // Get current test mode status
  socket.on('getTestModeStatus', () => {
    const status = getTestModeStatus();
    socket.emit('testModeStatus', {
      success: true,
      ...status
    });
  });

  socket.on('disconnect', () => {
    const participant = participants.get(socket.id);

    // Close streaming Deepgram connection
    const streamData = streamingConnections.get(socket.id);
    if (streamData && streamData.connection) {
      try {
        streamData.connection.finish();
        console.log(`[Streaming STT] Connection closed for ${participant?.username || socket.id}`);
      } catch (error) {
        console.error(`[Streaming STT] Error closing connection:`, error);
      }
      streamingConnections.delete(socket.id);

      // Clean up Hume AI connection
      const humeClient = humeConnections.get(socket.id);
      if (humeClient) {
        humeClient.disconnect();
        humeConnections.delete(socket.id);
      }
      humeAudioBuffers.delete(socket.id);
      socketToExtension.delete(socket.id);
    }

    if (participant) {
      const { roomId, username } = participant;
      const room = rooms.get(roomId);

      if (room) {
        room.participants.delete(socket.id);

        // Notify others
        socket.to(roomId).emit('participant-left', {
          participantId: socket.id,
          username
        });

        // Clean up empty rooms
        if (room.participants.size === 0) {
          rooms.delete(roomId);
          console.log(`Room ${roomId} deleted (empty)`);
        }
      }

      participants.delete(socket.id);
      console.log(`${username} disconnected from room ${roomId}`);
    }
  });
});

// API Routes
app.get('/', (req, res) => {
  res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

app.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    services: {
      deepgram: !!deepgramApiKey,
      deepl: !!deeplApiKey,
      elevenlabs: !!elevenlabsApiKey
    },
    activeRooms: rooms.size,
    activeParticipants: participants.size
  });
});

app.get('/api/languages', (req, res) => {
  res.json(languageMap);
});

// File listing API endpoints
app.get('/api/files/recordings', (req, res) => {
  const recordingsDir = path.join(__dirname, 'recordings');
  fs.readdir(recordingsDir, (err, files) => {
    if (err) {
      return res.json({ files: [] });
    }
    const fileList = files.map(filename => ({
      name: filename,
      path: `/files/recordings/${filename}`
    }));
    res.json({ files: fileList });
  });
});

app.get('/api/files/transcripts', (req, res) => {
  const transcriptsDir = path.join(__dirname, 'transcripts');
  fs.readdir(transcriptsDir, (err, files) => {
    if (err) {
      return res.json({ files: [] });
    }
    const fileList = files.map(filename => ({
      name: filename,
      path: `/files/transcripts/${filename}`
    }));
    res.json({ files: fileList });
  });
});

app.get('/api/files/translations', (req, res) => {
  const translationsDir = path.join(__dirname, 'translations');
  fs.readdir(translationsDir, (err, files) => {
    if (err) {
      return res.json({ files: [] });
    }
    const fileList = files.map(filename => ({
      name: filename,
      path: `/files/translations/${filename}`
    }));
    res.json({ files: fileList });
  });
});

// HMLCP API Endpoints
app.use(express.json());

// Get user profile stats
app.get('/api/hmlcp/profile/:userId/:language', async (req, res) => {
  try {
    const { userId, language } = req.params;
    const { profile, uloLayer } = await getUserProfile(userId, language);

    res.json({
      userId: profile.userId,
      language: profile.language,
      created: profile.created,
      lastUpdated: profile.lastUpdated,
      tone: profile.tone,
      avgSentenceLength: profile.avgSentenceLength,
      directness: profile.directness,
      ambiguityTolerance: profile.ambiguityTolerance,
      lexicalBias: profile.lexicalBias,
      phraseMappings: Object.keys(profile.phraseMap).length,
      biasTerms: profile.biasTerms.length,
      samples: profile.textSamples.length + profile.audioSamples.length,
      corrections: profile.corrections.length,
      metrics: profile.metrics,
      uloStats: uloLayer.getStats()
    });
  } catch (error) {
    console.error('[HMLCP API] Error getting profile:', error);
    res.status(500).json({ error: 'Failed to get profile' });
  }
});

// Submit a correction (for learning)
app.post('/api/hmlcp/correction', async (req, res) => {
  try {
    const { userId, language, rawInput, interpretedIntent, correctedIntent } = req.body;

    if (!userId || !language || !rawInput || !correctedIntent) {
      return res.status(400).json({ error: 'Missing required fields' });
    }

    const { profile, uloLayer } = await getUserProfile(userId, language);

    // Learn from correction
    uloLayer.learnFromCorrection(rawInput, interpretedIntent || rawInput, correctedIntent);

    // Save profile
    await profile.save();

    res.json({
      success: true,
      message: 'Correction learned',
      metrics: profile.metrics,
      newPhraseMappings: Object.keys(profile.phraseMap).length
    });
  } catch (error) {
    console.error('[HMLCP API] Error submitting correction:', error);
    res.status(500).json({ error: 'Failed to submit correction' });
  }
});

// Analyze profile patterns
app.post('/api/hmlcp/analyze', async (req, res) => {
  try {
    const { userId, language } = req.body;

    if (!userId || !language) {
      return res.status(400).json({ error: 'Missing userId or language' });
    }

    const { profile, patternExtractor } = await getUserProfile(userId, language);

    // Extract patterns from collected samples
    const patterns = patternExtractor.analyze(profile.textSamples);

    // Update profile with extracted patterns
    profile.tone = patterns.tone;
    profile.avgSentenceLength = patterns.avgSentenceLength;
    profile.directness = patterns.directness;
    profile.ambiguityTolerance = patterns.ambiguityTolerance;
    profile.lexicalBias = patterns.lexicalBias;

    // Save updated profile
    await profile.save();

    res.json({
      success: true,
      patterns,
      message: 'Profile patterns analyzed and updated'
    });
  } catch (error) {
    console.error('[HMLCP API] Error analyzing patterns:', error);
    res.status(500).json({ error: 'Failed to analyze patterns' });
  }
});

// Get custom vocabulary for Deepgram
app.get('/api/hmlcp/vocabulary/:userId/:language', async (req, res) => {
  try {
    const { userId, language } = req.params;
    const { uloLayer } = await getUserProfile(userId, language);

    const vocabulary = uloLayer.generateCustomVocabulary();

    res.json({
      success: true,
      vocabulary,
      count: vocabulary.length
    });
  } catch (error) {
    console.error('[HMLCP API] Error getting vocabulary:', error);
    res.status(500).json({ error: 'Failed to get vocabulary' });
  }
});

// Save profile manually
app.post('/api/hmlcp/save', async (req, res) => {
  try {
    const { userId, language } = req.body;

    if (!userId || !language) {
      return res.status(400).json({ error: 'Missing userId or language' });
    }

    const { profile } = await getUserProfile(userId, language);
    const filePath = await profile.save();

    res.json({
      success: true,
      message: 'Profile saved',
      filePath
    });
  } catch (error) {
    console.error('[HMLCP API] Error saving profile:', error);
    res.status(500).json({ error: 'Failed to save profile' });
  }
});


// ============================================================================
// UDP TRANSPORT FOR EXTENSIONS 3333 & 4444
// Copied from conf-server-phase1.js pattern for low-latency audio transport
// ============================================================================

const UDP_CONFIG = {
  port3333In: 6300,    // Receive PCM from gateway-3333
  port3333Out: 6301,   // Send translated PCM to gateway-3333
  port4444In: 6302,    // Receive PCM from gateway-4444
  port4444Out: 6303,   // Send translated PCM to gateway-4444
  gatewayHost: '127.0.0.1',
  frameSizeBytes: 640  // 20ms PCM16 @ 16kHz
};

// Create UDP sockets for audio transport
const socket3333In = dgram.createSocket('udp4');
const socket3333Out = dgram.createSocket('udp4');
const socket4444In = dgram.createSocket('udp4');
const socket4444Out = dgram.createSocket('udp4');

// Stats tracking for UDP
let udpStats = {
  from3333: 0,
  to3333: 0,
  from4444: 0,
  to4444: 0,
  startTime: Date.now()
};

// UDP message handler for Extension 3333 (receives English audio)
socket3333In.on('message', (msg, rinfo) => {
  udpStats.from3333++;
  
  if (udpStats.from3333 <= 3) {
    console.log(`[UDP] ‚úì Gateway-3333 connected: ${msg.length} bytes/frame`);
  }
  
  // Broadcast to Socket.IO for dashboard monitoring
  // This allows existing Socket.IO clients to visualize the audio
  global.io.emit('audioStream', {
    extension: '3333',
    buffer: msg,
    sampleRate: 16000,
    channels: 1,
    timestamp: Date.now(),
    transport: 'udp'
  });
});

// UDP message handler for Extension 4444 (receives French audio)
socket4444In.on('message', (msg, rinfo) => {
  udpStats.from4444++;
  
  if (udpStats.from4444 <= 3) {
    console.log(`[UDP] ‚úì Gateway-4444 connected: ${msg.length} bytes/frame`);
  }
  
  global.io.emit('audioStream', {
    extension: '4444',
    buffer: msg,
    sampleRate: 16000,
    channels: 1,
    timestamp: Date.now(),
    transport: 'udp'
  });
});

// Bind UDP sockets
socket3333In.bind(UDP_CONFIG.port3333In, () => {
  console.log(`[UDP] ‚úì Listening for Extension 3333 on UDP ${UDP_CONFIG.port3333In}`);
});

socket4444In.bind(UDP_CONFIG.port4444In, () => {
  console.log(`[UDP] ‚úì Listening for Extension 4444 on UDP ${UDP_CONFIG.port4444In}`);
});

// Stats logging every 30 seconds
setInterval(() => {
  const uptime = Math.floor((Date.now() - udpStats.startTime) / 1000);
  console.log(`[UDP Stats] Uptime=${uptime}s, RX_3333=${udpStats.from3333}, TX_3333=${udpStats.to3333}, RX_4444=${udpStats.from4444}, TX_4444=${udpStats.to4444}`);
}, 30000);

// Helper function to send translated audio back via UDP
function sendTranslatedAudioUDP(extension, audioBuffer) {
  if (extension === '3333') {
    socket3333Out.send(audioBuffer, UDP_CONFIG.port3333Out, UDP_CONFIG.gatewayHost, (err) => {
      if (!err) udpStats.to3333++;
    });
  } else if (extension === '4444') {
    socket4444Out.send(audioBuffer, UDP_CONFIG.port4444Out, UDP_CONFIG.gatewayHost, (err) => {
      if (!err) udpStats.to4444++;
    });
  }
}

// Make UDP sender globally available
global.sendTranslatedAudioUDP = sendTranslatedAudioUDP;

console.log('[UDP] ‚úì UDP transport layer initialized for Extensions 3333 & 4444');

// ============================================================================
const PORT = process.env.PORT || 3020;
const HOST = '0.0.0.0'; // Listen on all network interfaces

server.listen(PORT, HOST, () => {
  // Get local IP addresses
  const os = require('os');
  const networkInterfaces = os.networkInterfaces();
  const localIPs = [];

  Object.keys(networkInterfaces).forEach(interfaceName => {
    networkInterfaces[interfaceName].forEach(iface => {
      if (iface.family === 'IPv4' && !iface.internal) {
        localIPs.push(iface.address);
      }
    });
  });

  const protocol = server instanceof https.Server ? 'https' : 'http';

  console.log(`Conference server running on port ${PORT}`);
  console.log(`\nProtocol: ${protocol.toUpperCase()}`);
  console.log('\nAccess URLs:');
  console.log(`  - Local:    ${protocol}://localhost:${PORT}`);
  localIPs.forEach(ip => {
    console.log(`  - Network:  ${protocol}://${ip}:${PORT}`);
  });
  console.log('\nServices status:');
  console.log('  - Deepgram STT:', deepgramApiKey ? '‚úì' : '‚úó (not configured)');
  console.log('  - DeepL Translation:', deeplApiKey ? '‚úì' : '‚úó (not configured)');
  console.log('  - ElevenLabs TTS:', elevenlabsApiKey ? '‚úì' : '‚úó (not configured)');

  if (protocol === 'https') {
    console.log('\n‚úì HTTPS enabled - Microphone will work on remote devices!');
    console.log('  (You may need to click "Advanced" and accept the self-signed certificate)');
  } else {
    console.log('\n‚ö† HTTP only - Microphone will only work on localhost');
    console.log('  Add cert.pem and key.pem for HTTPS support');
  }
  console.log('\n‚úì Server is accessible from other devices on your network');

  // HMLCP: Periodic profile saving (every 5 minutes)
  setInterval(async () => {
    let savedCount = 0;
    for (const [key, { profile }] of userProfiles.entries()) {
      try {
        await profile.save();
        savedCount++;
      } catch (error) {
        console.error(`[HMLCP] Error saving profile ${key}:`, error.message);
      }
    }
    if (savedCount > 0) {
      console.log(`[HMLCP] Auto-saved ${savedCount} profile(s)`);
    }
  }, 5 * 60 * 1000); // 5 minutes

  console.log('\n[HMLCP] System initialized');
  console.log('  - User profiles: Auto-save enabled (every 5 min)');
  console.log('  - ULO Layer: Active for real-time adaptation');
  console.log('  - Pattern Extraction: Available via API');
});
