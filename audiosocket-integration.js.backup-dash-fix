// ============================================================================
// Complete AudioSocket Pipeline: STT â†’ Translation â†’ TTS (OPTIMIZED)
// Asterisk â†’ Deepgram â†’ DeepL â†’ ElevenLabs (PCM) â†’ Asterisk
// Uses PCM output from ElevenLabs (16kHz) with simple downsampling to 8kHz
// REFACTORED FOR DUAL-PROCESS SUPPORT (7000 and 7001)
// ============================================================================

// Load environment variables FIRST
require('dotenv').config();

// ============================================================================
// Comfort Noise Config Update Handler
// ============================================================================
function applyComfortNoiseConfig() {
    console.log('[audiosocket] applyComfortNoiseConfig() called');
    console.log('[audiosocket] global.comfortNoiseConfig:', JSON.stringify(global.comfortNoiseConfig));

    if (!global.comfortNoiseConfig) {
        console.log('[audiosocket] No comfort noise config to apply');
        return;
    }

    // Apply to all active audio stream buffers
    if (global.activeAudioStreamBuffers) {
        console.log('[audiosocket] Applying to', global.activeAudioStreamBuffers.size, 'active buffers');
        global.activeAudioStreamBuffers.forEach((buffer, participantId) => {
            console.log('[audiosocket] Updating buffer for participant:', participantId);
            buffer.updateComfortNoiseConfig(global.comfortNoiseConfig);

            if (global.comfortNoiseConfig.bufferDelay !== undefined) {
                console.log('[audiosocket] Setting delay to:', global.comfortNoiseConfig.bufferDelay, 'ms');
                buffer.setDelay(global.comfortNoiseConfig.bufferDelay);
            }
        });
        console.log('[audiosocket] âœ“ Config applied to all active buffers');
    } else {
        console.log('[audiosocket] No active buffers - config will apply on next call');
    }
}

// Export globally so conference-server can call it
global.applyComfortNoiseConfig = applyComfortNoiseConfig;

// Track active audio stream buffers
if (!global.activeAudioStreamBuffers) {
    global.activeAudioStreamBuffers = new Map();
}



const AudioSocketOrchestrator = require('./audiosocket-orchestrator');
const { ASRStreamingWorker } = require('./asr-streaming-worker');
const { DeepLIncrementalMT } = require('./deepl-incremental-mt');  // Destructure the export
const ElevenLabsTTSService = require('./elevenlabs-tts-service');

const HumeStreamingClient = require('./hume-streaming-client');
const AudioStreamBuffer = require('./audio-stream-buffer');
const WebSocket = require('ws');

// Get API keys from environment
const deepgramApiKey = process.env.DEEPGRAM_API_KEY;
const deeplApiKey = process.env.DEEPL_API_KEY;
const elevenlabsApiKey = process.env.ELEVENLABS_API_KEY;
const humeApiKey = process.env.HUME_EVI_API_KEY;

console.log('[Pipeline] Initializing complete translation pipeline...');
console.log('[Pipeline] Deepgram:', deepgramApiKey ? 'âœ“' : 'âœ—');
console.log('[Pipeline] DeepL:', deeplApiKey ? 'âœ“' : 'âœ—');
console.log('[Pipeline] ElevenLabs:', elevenlabsApiKey ? 'âœ“' : 'âœ—');
console.log('[Pipeline] Hume AI:', humeApiKey ? 'âœ“' : 'âœ—');

// Initialize AudioSocket orchestrator (listens on port 5050 for Asterisk)
const audioSocketOrchestrator = new AudioSocketOrchestrator(5050);

// Initialize translation services (shared across all calls)
const translator = deeplApiKey ? new DeepLIncrementalMT(deeplApiKey) : null;
const ttsService = elevenlabsApiKey ? new ElevenLabsTTSService(elevenlabsApiKey) : null;

// ============================================================================
// MULTI-PROCESS SESSION MANAGEMENT
// Each call (7000-xxx, 7001-xxx) gets its own session with dedicated workers
// ============================================================================
const activeSessions = new Map();

/**
 * Parse extension number from UUID
 * Format: "7000-xxxxx" or "7001-xxxxx"
 * Returns: "7000" or "7001"
 */
function getExtensionFromUUID(uuid) {
    if (!uuid) return null;
    const parts = uuid.split('-');
    return parts[0]; // Returns "7000" or "7001"
}

/**
 * Get or create session for a given UUID
 */
function getSession(uuid) {
    if (!activeSessions.has(uuid)) {
        console.log('[Pipeline] Creating new session for:', uuid);
        const extension = getExtensionFromUUID(uuid);
        activeSessions.set(uuid, {
            uuid: uuid,
            extension: extension,
            asrWorker: null,
            humeWorker: null,
            audioStreamBuffer: null,
            micWebSocket: null,
            humeAudioBuffer: [],
            created: Date.now()
        });
    }
    return activeSessions.get(uuid);
}

/**
 * Remove session
 */
function removeSession(uuid) {
    const session = activeSessions.get(uuid);
    if (session) {
        // Clean up resources
        if (session.micWebSocket) {
            session.micWebSocket.close();
        }
        if (session.asrWorker) {
            session.asrWorker.disconnect();
        }
        if (session.humeWorker) {
            session.humeWorker.disconnect();
        }
        if (session.audioStreamBuffer && global.activeAudioStreamBuffers) {
            global.activeAudioStreamBuffers.delete(uuid);
        }

        activeSessions.delete(uuid);
        console.log('[Pipeline] âœ“ Session removed:', uuid, '| Active sessions:', activeSessions.size);
    }
}

// ElevenLabs voice ID (TODO: make this configurable)
const VOICE_ID = 'pNInz6obpgDQGcFmaJgB';  // Default voice (Adam)

// Get Socket.IO instance from global (set by conference-server.js)
const getIO = () => global.io;

// Translation configuration (TODO: make this dynamic per session)
// Language configuration - now dynamic from QA Settings
function getSourceLang() {
    return (global.qaConfig && global.qaConfig.sourceLang) || 'en';
}
function getTargetLang() {
    return (global.qaConfig && global.qaConfig.targetLang) || 'ja';
}

/**
 * Downsample PCM audio from 16kHz to 8kHz
 * Simple decimation: keep every other sample
 * Much faster than ffmpeg conversion!
 */
function downsamplePCM16to8(pcm16Buffer) {
    const samples16 = pcm16Buffer.length / 2; // 16-bit = 2 bytes per sample
    const samples8 = Math.floor(samples16 / 2);
    const pcm8Buffer = Buffer.alloc(samples8 * 2);

    for (let i = 0; i < samples8; i++) {
        // Copy every other sample (simple decimation)
        const srcOffset = i * 4; // Every 2nd sample in 16kHz
        const dstOffset = i * 2;
        pcm16Buffer.copy(pcm8Buffer, dstOffset, srcOffset, srcOffset + 2);
    }

    return pcm8Buffer;
}

/**
 * Send audio to WebSocket mic endpoint in proper frames
 * WebSocket expects raw PCM binary data in 640-byte frames (16kHz, 20ms)
 */
function sendAudioToMicEndpoint(micWebSocket, pcmBuffer) {
    if (!micWebSocket || micWebSocket.readyState !== WebSocket.OPEN) {
        console.warn('[MicWebSocket] Not connected, cannot send audio');
        return;
    }

    const FRAME_SIZE = 640; // 16kHz * 20ms * 2 bytes = 640 bytes per frame
    const numFrames = Math.floor(pcmBuffer.length / FRAME_SIZE);

    for (let i = 0; i < numFrames; i++) {
        const frame = pcmBuffer.slice(i * FRAME_SIZE, (i + 1) * FRAME_SIZE);
        micWebSocket.send(frame);
    }

    // Send remaining partial frame if any
    if (pcmBuffer.length % FRAME_SIZE !== 0) {
        const remainingFrame = pcmBuffer.slice(numFrames * FRAME_SIZE);
        micWebSocket.send(remainingFrame);
    }
}

// ============================================================================
// Initialize Audio Stream Buffer (per call, per language)
// ============================================================================
function initializeAudioStreamBuffer(uuid) {
    const session = getSession(uuid);

    if (session.audioStreamBuffer) {
        console.log('[Pipeline] AudioStreamBuffer already initialized for', uuid);
        return session.audioStreamBuffer;
    }

    console.log('[Pipeline] Initializing AudioStreamBuffer for:', uuid);

    // Create AudioStreamBuffer with 16kHz (matches WebSocket mic endpoint requirement)
    session.audioStreamBuffer = new AudioStreamBuffer({
        sampleRate: 16000,  // 16kHz to match WebSocket mic endpoint
        channels: 1,
        bitDepth: 16,
        maxBufferSize: 2000  // 2 second max buffer
    });

    // Enable comfort noise by default
    const defaultComfortNoiseConfig = {
        enabled: true,  // Enable by default
        noiseType: 'white',
        speechLevel: -30,
        silenceLevel: -15,
        vadThreshold: 0.01,
        fadeInMs: 50,
        fadeOutMs: 50,
        bufferDelay: 0
    };

    // Apply global comfort noise config if available, otherwise use defaults
    const configToApply = global.comfortNoiseConfig || defaultComfortNoiseConfig;
    console.log('[Pipeline] Applying comfort noise config:', configToApply);
    session.audioStreamBuffer.updateComfortNoiseConfig(configToApply);

    if (configToApply.bufferDelay !== undefined) {
        session.audioStreamBuffer.setDelay(configToApply.bufferDelay);
    }

    // Register in global tracking Map
    if (global.activeAudioStreamBuffers) {
        global.activeAudioStreamBuffers.set(uuid, session.audioStreamBuffer);
        console.log('[Pipeline] âœ“ Registered buffer in global.activeAudioStreamBuffers');
        console.log('[Pipeline] Active buffers:', global.activeAudioStreamBuffers.size);
    }

    // Create WebSocket connection to mic endpoint
    const micEndpointUrl = `ws://127.0.0.1:5051/mic/${uuid}`;
    console.log('[MicWebSocket] Connecting to:', micEndpointUrl);

    session.micWebSocket = new WebSocket(micEndpointUrl);

    session.micWebSocket.on('open', () => {
        console.log('[MicWebSocket] âœ“ Connected to mic endpoint for', uuid);
        console.log('[MicWebSocket] Audio will be injected as microphone input');
    });

    session.micWebSocket.on('error', (error) => {
        console.error('[MicWebSocket] âœ— Connection error for', uuid, ':', error.message);
    });

    session.micWebSocket.on('close', () => {
        console.log('[MicWebSocket] Disconnected from mic endpoint for', uuid);
    });

    // Listen for processed audio from buffer and send to mic endpoint (NOT speaker)
    session.audioStreamBuffer.on('audioReady', (audioData) => {
        // Extract buffer from audioData object {buffer, metadata, actualDelay}
        const pcmBuffer = audioData.buffer;

        // Send to WebSocket mic endpoint instead of AudioSocket speaker
        sendAudioToMicEndpoint(session.micWebSocket, pcmBuffer);

        console.log('[Pipeline] âœ“ Audio sent to mic channel for', uuid, '(16kHz,', pcmBuffer.length, 'bytes)');
    });

    console.log('[Pipeline] âœ“ AudioStreamBuffer initialized for', uuid, '(16kHz, comfort noise enabled)');
    return session.audioStreamBuffer;
}

async function initializeHumeWorker(uuid) {
    const session = getSession(uuid);

    if (session.humeWorker) {
        console.log('[Pipeline] Hume worker already initialized for', uuid);
        return session.humeWorker;
    }

    if (!humeApiKey) {
        console.warn('[Hume] No API key, emotion detection disabled');
        return null;
    }

    try {
        session.humeWorker = new HumeStreamingClient(humeApiKey, {
            sampleRate: 8000,
            channels: 1
        });
        await session.humeWorker.connect();
        console.log('[Hume] âœ“ Emotion detection worker connected for', uuid);

        // Emit emotion metrics via Socket.IO (with extension filter)
        session.humeWorker.on('metrics', (metrics) => {
            const io = getIO();
            if (io) {
                io.emit('emotion_detected', {
                    extension: session.extension,
                    uuid: uuid,
                    arousal: metrics.arousal,
                    valence: metrics.valence,
                    energy: metrics.energy,
                    timestamp: metrics.timestamp
                });
            }
        });

        return session.humeWorker;
    } catch (error) {
        console.error('[Hume] Error initializing worker for', uuid, ':', error.message);
        return null;
    }
}


async function initializeASRWorker(uuid) {
    const session = getSession(uuid);

    if (session.asrWorker && session.asrWorker.connected) {
        console.log('[Pipeline] ASR worker already connected for', uuid);
        return session.asrWorker;
    }

    if (!deepgramApiKey) {
        console.warn('[Pipeline] No Deepgram API key, ASR disabled');
        return null;
    }

    try {
        session.asrWorker = new ASRStreamingWorker(deepgramApiKey, getSourceLang());
        await session.asrWorker.connect();
        console.log('[Pipeline] âœ“ ASR worker connected for', uuid);

        // Handle PARTIAL transcripts (real-time feedback)
        session.asrWorker.on('partial', (transcript) => {
            console.log('[Pipeline]', uuid, 'Partial:', transcript.text);
            const io = getIO();
            if (io) {
                io.emit('transcriptionPartial', {
                    extension: session.extension,
                    connectionId: uuid,
                    uuid: uuid,
                    text: transcript.text,
                    language: transcript.language,
                    type: 'partial'
                });
            }
        });

        // Handle FINAL transcripts (trigger translation pipeline)
        session.asrWorker.on('final', async (transcript) => {
            console.log('[Pipeline]', uuid, 'Final:', transcript.text);
            const asrEndTime = performance.now();  // Capture ASR end time for network latency measurement

            // Send transcript to browser
            const io = getIO();
            if (io) {
                io.emit('transcriptionFinal', {
                    extension: session.extension,
                    connectionId: uuid,
                    uuid: uuid,
                    text: transcript.text,
                    transcript: transcript.text,
                    language: transcript.language,
                    confidence: transcript.confidence,
                    type: 'final',
                    latency: transcript.latency || 100
                });
            }

            // ========================================
            // TRANSLATION PIPELINE STARTS HERE
            // ========================================
            await processTranslationPipeline(uuid, transcript.text, asrEndTime);
        });

        return session.asrWorker;
    } catch (err) {
        console.error('[Pipeline] ASR initialization failed for', uuid, ':', err.message);
        return null;
    }
}

/**
 * Process complete translation pipeline:
 * 1. Translate text (DeepL)
 * 2. Synthesize audio as PCM 16kHz (ElevenLabs)
 * 3. Downsample to PCM 8kHz
 * 4. Send back to Asterisk
 */
async function processTranslationPipeline(uuid, originalText, asrEndTime) {
    const session = getSession(uuid);
    const pipelineStart = performance.now();

    console.log('[Pipeline]', uuid, 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('[Pipeline]', uuid, 'Starting translation pipeline');
    console.log('[Pipeline]', uuid, 'Original text:', originalText);

try {
        // Step 1: Translate with DeepL (or bypass if source === target)
        if (!translator) {
            console.error('[Pipeline]', uuid, 'DeepL not initialized, skipping translation');
            return;
        }

        const sourceLang = getSourceLang();
        const targetLang = getTargetLang();

        console.log(`[Pipeline] ${uuid} [1/4] Translation check: ${sourceLang} â†’ ${targetLang}`);
        const translationStart = performance.now();
        const asrToMtNetwork = translationStart - asrEndTime;  // Network latency: ASR end â†’ MT start
        console.log(`[TIMING-VERIFY] ${uuid} ASR ended at ${asrEndTime}, MT started at ${translationStart}, Network: ${asrToMtNetwork}ms`);

        let translationResult;
        let translationTime;

        // QA Mode: Bypass DeepL if source === target
        if (sourceLang === targetLang) {
            console.log('[Pipeline]', uuid, 'âš ï¸  QA Mode Active: Bypassing DeepL translation (same language)');
            translationResult = { text: originalText };
            translationTime = 0;
        } else {
            console.log('[Pipeline]', uuid, 'Calling DeepL for translation...');
            translationResult = await translator.translateIncremental(
                uuid, // Use full UUID as session ID
                sourceLang,
                targetLang,
                originalText,
                true
            );
            translationTime = Date.now() - translationStart;
        }

        console.log('[Pipeline]', uuid, 'âœ“ Translation complete:', translationResult.text);
        console.log('[Pipeline]', uuid, '  Time:', translationTime, 'ms');

        // Send translation to browser
        const io = getIO();
        if (io) {
            io.emit('translationComplete', {
                extension: session.extension,
                connectionId: uuid,
                uuid: uuid,
                original: originalText,
                translation: translationResult.text,
                sourceLang: sourceLang,
                targetLang: targetLang,
                time: translationTime
            });
        }

        // Step 2: Synthesize with ElevenLabs (PCM 16kHz output)
        if (!ttsService) {
            console.error('[Pipeline]', uuid, 'ElevenLabs not initialized, skipping TTS');
            return;
        }

        console.log('[Pipeline]', uuid, '[2/4] Synthesizing speech (PCM 16kHz)...');
        const ttsStart = performance.now();
        const mtToTtsNetwork = ttsStart - (translationStart + translationTime);  // Network latency: MT end â†’ TTS start
        console.log(`[TIMING-VERIFY] ${uuid} MT: ${translationStart} + ${translationTime} = ${translationStart + translationTime}, TTS started at ${ttsStart}, Network: ${mtToTtsNetwork}ms`);

        // Override the synthesize method to use PCM output
        const axios = require('axios');
        const response = await axios.post(
            `${ttsService.baseURL}/text-to-speech/${VOICE_ID}`,
            {
                text: translationResult.text,
                model_id: 'eleven_multilingual_v2',
                voice_settings: {
                    stability: 0.5,
                    similarity_boost: 0.75,
                    use_speaker_boost: true
                },
                output_format: "pcm_16000"  // MP3 format
            },
            {
                headers: {
                    'xi-api-key': ttsService.apiKey,
                    'Content-Type': 'application/json'
                },
                responseType: 'arraybuffer'
            }
        );

        const pcm16Buffer = Buffer.from(response.data);
        const ttsTime = performance.now() - ttsStart;

        console.log('[Pipeline]', uuid, 'âœ“ TTS complete');
        console.log('[Pipeline]', uuid, '  Audio size:', pcm16Buffer.length, 'bytes');
        console.log('[Pipeline]', uuid, '  Format: PCM 16kHz S16LE');
        console.log('[Pipeline]', uuid, '  Duration:', (pcm16Buffer.length / 2 / 16000).toFixed(2), 'seconds');
        console.log('[Pipeline]', uuid, '  Time:', ttsTime, 'ms');

        // Save ElevenLabs output to file for debugging
        const fs = require('fs');
        const timestamp = Date.now();
        const recordingPath = `./recordings/elevenlabs-${uuid}-${timestamp}-16khz.pcm`;
        try {
            fs.writeFileSync(recordingPath, pcm16Buffer);
            console.log('[Pipeline]', uuid, 'ðŸ’¾ Saved ElevenLabs output:', recordingPath);
        } catch (err) {
            console.error('[Pipeline]', uuid, 'Failed to save recording:', err.message);
        }

        // Step 3: Skip downsampling - use 16kHz directly for mic endpoint
        console.log('[Pipeline]', uuid, '[3/4] Using 16kHz PCM directly (no downsampling needed)');
        const convertStart = performance.now();

        const ttsToBufferNetwork = convertStart - (ttsStart + ttsTime);  // Network latency: TTS end â†’ Buffer start
        console.log(`[TIMING-VERIFY] ${uuid} TTS: ${ttsStart} + ${ttsTime} = ${ttsStart + ttsTime}, Buffer started at ${convertStart}, Network: ${ttsToBufferNetwork}ms`);

        const convertTime = 0; // No conversion needed
        console.log('[Pipeline]', uuid, 'âœ“ Audio ready for buffer (16kHz)');
        console.log('[Pipeline]', uuid, '  PCM 16kHz size:', pcm16Buffer.length, 'bytes');
        console.log('[Pipeline]', uuid, '  Audio duration:', (pcm16Buffer.length / 2 / 16000).toFixed(2), 'seconds');

        // Send translated audio to browser for playback (16kHz for better quality)
        if (io) {
            console.log('[Pipeline]', uuid, 'ðŸ“¤ Sending translated audio to browser...');
            io.emit('translatedAudio', {
                extension: session.extension,
                uuid: uuid,
                audio: pcm16Buffer.toString("base64"),  // Send as Buffer (same as audioStream)
                sampleRate: 16000,  // 16kHz PCM from ElevenLabs
                channels: 1,
                bitDepth: 16,
                format: "pcm",
                translation: translationResult.text,
                original: originalText,
                duration: (pcm16Buffer.length / 2 / 16000).toFixed(2),
                timestamp: Date.now()
            });
            console.log('[Pipeline]', uuid, 'âœ“ Sent audio to browser:', pcm16Buffer.length, 'bytes');
        }

        // Step 4: Send PCM audio to mic endpoint via AudioStreamBuffer
        if (!session.audioStreamBuffer) {
            console.error('[Pipeline]', uuid, 'No AudioStreamBuffer for this session');
            return;
        }

        console.log('[Pipeline]', uuid, '[4/4] Sending audio through AudioStreamBuffer to mic endpoint...');
        const sendStart = performance.now();

        const bufferToSendNetwork = sendStart - convertStart;  // Network latency: Buffer start â†’ Send start
        console.log(`[TIMING-VERIFY] ${uuid} Buffer: ${convertStart}, Send started at ${sendStart}, Network: ${bufferToSendNetwork}ms`);

        // Route through AudioStreamBuffer for comfort noise and delay
        console.log('[Pipeline]', uuid, 'Routing 16kHz audio through AudioStreamBuffer');
        session.audioStreamBuffer.addAudioChunk(pcm16Buffer);  // Send 16kHz buffer directly
        console.log('[Pipeline]', uuid, 'âœ“ Audio queued in buffer (will be sent to mic endpoint)');

        const sendTime = performance.now() - sendStart;
        console.log('[Pipeline]', uuid, 'âœ“ Audio processing time:', sendTime, 'ms');

        // Calculate total pipeline time
        const totalTime = performance.now() - pipelineStart;
        console.log('[Pipeline]', uuid, 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
        console.log('[Pipeline]', uuid, 'Pipeline complete!');
        console.log('[Pipeline]', uuid, 'Total time:', totalTime, 'ms');
        console.log('[Pipeline]', uuid, '  - Translation:', translationTime, 'ms');
        console.log('[Pipeline]', uuid, '  - TTS (PCM):', ttsTime, 'ms');
        console.log('[Pipeline]', uuid, '  - Buffer:', convertTime, 'ms');
        console.log('[Pipeline]', uuid, '  - Send:', sendTime, 'ms');
        console.log('[Pipeline]', uuid, 'â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

        // Send pipeline stats to browser
        if (io) {
            io.emit('pipelineComplete', {
                extension: session.extension,
                uuid: uuid,
                original: originalText,
                translation: translationResult.text,
                totalTime,
                translationTime,
                ttsTime,
                convertTime,
                sendTime,
                audioSize: pcm16Buffer.length,
                audioDuration: (pcm16Buffer.length / 2 / 16000).toFixed(2),
                humeTime: 85,  // Parallel processing (typical emotion detection time) - does not block pipeline
                asrToMtNetwork,               // Network latency: ASR â†’ MT
                mtToTtsNetwork,               // Network latency: MT â†’ TTS
                ttsToBufferNetwork,           // Network latency: TTS â†’ Buffer
                bufferToSendNetwork           // Network latency: Buffer â†’ Send
            });
        }

    } catch (error) {
        console.error('[Pipeline]', uuid, 'âœ— Pipeline error:', error.message);
        console.error('[Pipeline]', uuid, '  Stack:', error.stack);

        const io = getIO();
        if (io) {
            io.emit('pipelineError', {
                extension: session.extension,
                uuid: uuid,
                error: error.message,
                original: originalText
            });
        }
    }
}

// Hume AI audio buffering constant
const HUME_BUFFER_SIZE = 50; // 50 frames = 1 second at 20ms per frame

// Handle incoming PCM frames from AudioSocket
audioSocketOrchestrator.on('pcm-frame', async (frame) => {
    const uuid = frame.connectionId || frame.uuid;
    const session = getSession(uuid);

    // Initialize ASR and Hume workers on first frame
    if (!session.asrWorker) {
        await initializeASRWorker(uuid);
        await initializeHumeWorker(uuid);
    }

    // Send frame to Deepgram for transcription
    if (session.asrWorker && session.asrWorker.connected) {
        session.asrWorker.sendAudio(frame.pcm, {
            segmentId: frame.sequenceNumber,
            duration: frame.duration
        });
    }

    // Fork audio to Hume AI for emotion detection (buffered for speech detection)
    if (session.humeWorker && session.humeWorker.connected) {
        session.humeAudioBuffer.push(frame.pcm);

        // Send buffered audio once we have 1 second (50 frames Ã— 20ms)
        if (session.humeAudioBuffer.length >= HUME_BUFFER_SIZE) {
            const combinedBuffer = Buffer.concat(session.humeAudioBuffer);
            session.humeWorker.sendAudio(combinedBuffer);
            console.log(`[Hume] ${uuid} Sent ${HUME_BUFFER_SIZE} frames (${combinedBuffer.length} bytes, 1 second buffer)`);
            session.humeAudioBuffer = []; // Reset buffer for next batch
        }
    }

    // FORK AUDIO TO BROWSER: Send same audio to Socket.IO clients for playback
    const io = getIO();
    if (io) {
        io.emit('audioStream', {
            extension: session.extension,
            uuid: uuid,
            buffer: frame.pcm,
            sequenceNumber: frame.sequenceNumber,
            sampleRate: 8000,
            channels: 1,
            bitDepth: 16,
            timestamp: Date.now()
        });
    }
});

// Log connections
audioSocketOrchestrator.on('connection', (info) => {
    const uuid = info.connectionId;
    const extension = getExtensionFromUUID(uuid);

    console.log('[Pipeline] âœ“ Asterisk connected:', uuid, '| Extension:', extension);

    // Create new session
    const session = getSession(uuid);

    // Initialize AudioStreamBuffer for this call
    initializeAudioStreamBuffer(uuid);

    // Translation session initialized (DeepL doesn't need explicit session creation)
    console.log('[Pipeline] âœ“ Translation ready for session:', uuid);
    console.log('[Pipeline] Active sessions:', activeSessions.size);

    const io = getIO();
    if (io) {
        io.emit('audiosocket-connected', {
            extension: extension,
            connectionId: uuid,
            uuid: uuid,
            timestamp: Date.now()
        });

        io.emit('audioStreamStart', {
            extension: extension,
            connectionId: uuid,
            format: {
                sampleRate: 8000,
                channels: 1,
                bitDepth: 16,
                encoding: 'pcm'
            }
        });
    }
});

audioSocketOrchestrator.on('handshake', (info) => {
    const uuid = info.uuid || info.connectionId;
    const extension = getExtensionFromUUID(uuid);

    console.log('[Pipeline] âœ“ Handshake complete:', uuid, '| Extension:', extension);

    const io = getIO();
    if (io) {
        io.emit('audiosocket-handshake', {
            extension: extension,
            uuid: uuid,
            connectionId: info.connectionId || uuid,
            timestamp: Date.now()
        });
    }
});

audioSocketOrchestrator.on('disconnect', (info) => {
    const uuid = info.connectionId || info.uuid;
    const extension = getExtensionFromUUID(uuid);

    console.log('[Pipeline] Asterisk disconnected:', {
        uuid: uuid,
        extension: extension,
        duration: `${info.duration.toFixed(1)}s`,
        frames: info.framesReceived
    });

    // Clean up session
    removeSession(uuid);

    const io = getIO();
    if (io) {
        io.emit('audiosocket-disconnected', {
            extension: extension,
            connectionId: uuid,
            uuid: uuid,
            duration: info.duration,
            frames: info.framesReceived,
            timestamp: Date.now()
        });

        io.emit('audioStreamEnd', {
            extension: extension,
            connectionId: uuid,
            duration: info.duration,
            frames: info.framesReceived
        });
    }
});

// Start orchestrator
audioSocketOrchestrator.start();

// Make orchestrator available globally
global.audioSocketOrchestrator = audioSocketOrchestrator;

// Clean up on shutdown
process.on('SIGTERM', () => {
    if (audioSocketOrchestrator) {
        audioSocketOrchestrator.stop();
    }
    // Clean up all sessions
    activeSessions.forEach((session, uuid) => {
        removeSession(uuid);
    });
});

process.on('SIGINT', () => {
    if (audioSocketOrchestrator) {
        audioSocketOrchestrator.stop();
    }
    // Clean up all sessions
    activeSessions.forEach((session, uuid) => {
        removeSession(uuid);
    });
});

console.log('[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
console.log('[Pipeline] Complete Translation Pipeline Initialized');
console.log('[Pipeline] MULTI-PROCESS MODE: Supports 7000 and 7001 simultaneously');
console.log('[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
console.log('[Pipeline] Flow: Asterisk â†’ Deepgram STT â†’ DeepL MT â†’ ElevenLabs TTS (PCM 16kHz) â†’ AudioStreamBuffer â†’ WebSocket Mic Endpoint');
console.log('[Pipeline] AudioSocket: port 5050 (receives audio FROM caller mic)');
console.log('[Pipeline] WebSocket Mic: port 5051 (sends audio TO caller mic as input)');
console.log('[Pipeline] Languages:', getSourceLang(), 'â†’', getTargetLang());
console.log('[Pipeline] Voice ID:', VOICE_ID);
console.log('[Pipeline] Audio: 16kHz PCM (with comfort noise) â†’ Mic endpoint');
console.log('[Pipeline] Comfort Noise: ENABLED by default');
console.log('[Pipeline] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

// Log Socket.IO status after delay
setTimeout(() => {
    const io = getIO();
    console.log('[Pipeline] Socket.IO:', !!io ? 'AVAILABLE âœ“' : 'NOT AVAILABLE âœ—');
}, 2000);
