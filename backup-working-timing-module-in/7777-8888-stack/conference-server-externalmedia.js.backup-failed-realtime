const express = require('express');
const http = require('http');
const https = require('https');
const fs = require('fs');
const socketIo = require('socket.io');
const path = require('path');
const { v4: uuidv4 } = require('uuid');
require('dotenv').config({ path: '.env.externalmedia' });

// Import translation services
const { createClient } = require('@deepgram/sdk');
const deepl = require('deepl-node');
// const sdk = require('microsoft-cognitiveservices-speech-sdk'); // Replaced with ElevenLabs
const ElevenLabsTTSService = require('./elevenlabs-tts-service');
const HumeStreamingClient = require('./hume-streaming-client');

// Import HMLCP modules
const { UserProfile, ULOLayer, PatternExtractor } = require('./hmlcp');
const { applyDefaultProfile } = require('./hmlcp/default-profiles');


const app = express();

// Create HTTPS server if certificates exist, otherwise HTTP
// But always use HTTP in Azure App Service (Azure handles SSL termination)
let server;
const isAzure = !!process.env.WEBSITE_INSTANCE_ID;

if (isAzure) {
  // Azure App Service - use HTTP (Azure handles HTTPS)
  server = http.createServer(app);
  console.log('âœ“ HTTP server for Azure App Service (Azure handles HTTPS termination)');
} else {
  try {
    const certPath = path.join(__dirname, 'cert.pem');
    const keyPath = path.join(__dirname, 'key.pem');

    if (fs.existsSync(certPath) && fs.existsSync(keyPath)) {
      const options = {
        key: fs.readFileSync(keyPath),
        cert: fs.readFileSync(certPath)
      };
      server = https.createServer(options, app);
      console.log('âœ“ HTTPS server configured with SSL certificates');
    } else {
      server = http.createServer(app);
      console.log('âš  HTTP server (certificates not found)');
    }
  } catch (error) {
    console.error('Error loading certificates, falling back to HTTP:', error.message);
    server = http.createServer(app);
  }
}

const io = socketIo(server, {
  cors: {
    origin: "*",
    methods: ["GET", "POST"]
  },
  maxHttpBufferSize: 1e8 // 100 MB for audio chunks
});

// Make Socket.IO available globally for audiosocket-integration
global.io = io;

// ============================================================================
// TIMING & BUFFERING MODULE - PHASE 1: ExtensionPairManager
// ============================================================================

/**
 * ExtensionPairManager - Manages 7777â†”8888 extension pairing
 * Tracks which extensions are paired for bidirectional translation
 */
class ExtensionPairManager {
  constructor() {
    this.pairs = new Map(); // ext â†’ pairedExt
    this.startTimes = new Map(); // ext â†’ callStartTime
  }

  registerPair(ext1, ext2) {
    this.pairs.set(ext1, ext2);
    this.pairs.set(ext2, ext1);
    this.startTimes.set(ext1, Date.now());
    this.startTimes.set(ext2, Date.now());
    console.log(`[PairManager] âœ“ Registered pair: ${ext1} â†” ${ext2}`);
  }

  isPaired(ext) {
    return this.pairs.has(ext);
  }

  getPairedExtension(ext) {
    return this.pairs.get(ext);
  }

  unregisterPair(ext) {
    const paired = this.pairs.get(ext);
    if (paired) {
      this.pairs.delete(ext);
      this.pairs.delete(paired);
      this.startTimes.delete(ext);
      this.startTimes.delete(paired);
      console.log(`[PairManager] âœ“ Unregistered pair: ${ext} â†” ${paired}`);
    }
  }

  getCallDuration(ext) {
    const startTime = this.startTimes.get(ext);
    if (!startTime) return 0;
    return Date.now() - startTime;
  }
}

// Initialize ExtensionPairManager globally
global.pairManager = new ExtensionPairManager();
// Auto-pair 7777 and 8888 on startup
global.pairManager.registerPair('7777', '8888');

// ============================================================================
// TIMING & BUFFERING MODULE - PHASE 2: LatencyTracker
// ============================================================================

/**
 * LatencyTracker - Tracks rolling average latencies for all pipeline stages
 * Maintains last 10 samples per direction and per stage
 */
class LatencyTracker {
  constructor() {
    this.directionSamples = new Map(); // 'ext1â†’ext2' â†’ [sample1, sample2, ...]
    this.stageSamples = new Map(); // 'ext:stage' â†’ [sample1, sample2, ...]
    this.maxSamples = 10; // Rolling average window
  }

  updateLatency(direction, latencyMs) {
    if (!this.directionSamples.has(direction)) {
      this.directionSamples.set(direction, []);
    }
    const samples = this.directionSamples.get(direction);
    samples.push(latencyMs);
    if (samples.length > this.maxSamples) {
      samples.shift();
    }
    const avg = this.getAverageLatency(direction);
    console.log(`[Latency] ${direction} = ${latencyMs}ms (avg: ${Math.round(avg)}ms, n=${samples.length})`);
  }

  updateStageLatency(extension, stageName, latencyMs) {
    const key = `${extension}:${stageName}`;
    if (!this.stageSamples.has(key)) {
      this.stageSamples.set(key, []);
    }
    const samples = this.stageSamples.get(key);
    samples.push(latencyMs);
    if (samples.length > this.maxSamples) {
      samples.shift();
    }
  }

  getAverageLatency(direction) {
    const samples = this.directionSamples.get(direction) || [];
    if (samples.length === 0) return 0;
    return samples.reduce((a, b) => a + b) / samples.length;
  }

  getStageAverage(extension, stageName) {
    const key = `${extension}:${stageName}`;
    const samples = this.stageSamples.get(key) || [];
    if (samples.length === 0) return 0;
    return samples.reduce((a, b) => a + b) / samples.length;
  }

  getLatencyDifference(ext1, ext2) {
    const direction1 = `${ext1}â†’${ext2}`;
    const direction2 = `${ext2}â†’${ext1}`;
    const avg1 = this.getAverageLatency(direction1);
    const avg2 = this.getAverageLatency(direction2);
    const difference = avg1 - avg2;
    console.log(`[LatencyDiff] ${direction1}=${Math.round(avg1)}ms, ${direction2}=${Math.round(avg2)}ms, Î”=${Math.round(difference)}ms`);
    return difference;
  }

  getAllLatencies(extension) {
    // 7777/8888 stack uses simplified 6-stage pipeline (not 9-stage like 7000/7001)
    const stages = [
      'gateway_to_asr', 'asr', 'asr_to_mt', 'mt', 'mt_to_tts', 'tts'
    ];
    const result = {};
    for (const stage of stages) {
      const key = `${extension}:${stage}`;
      const samples = this.stageSamples.get(key) || [];
      result[stage] = {
        current: samples[samples.length - 1] || 0,
        avg: this.getStageAverage(extension, stage)
      };
    }
    return result;
  }
}

// Initialize LatencyTracker globally
global.latencyTracker = new LatencyTracker();

// ============================================================================
// TIMING & BUFFERING MODULE - PHASE 3: AudioBufferManager
// ============================================================================

/**
 * AudioBufferManager - Manages buffered audio packets for synchronization
 * Delays faster translation direction to match slower direction
 */
class AudioBufferManager {
  constructor() {
    this.buffers = new Map(); // extension â†’ [{audioData, timestamp, delayMs}, ...]
    this.maxBufferSize = 100; // Maximum packets per extension
  }

  addToBuffer(extension, audioData, delayMs) {
    if (!this.buffers.has(extension)) {
      this.buffers.set(extension, []);
    }

    const buffer = this.buffers.get(extension);
    buffer.push({
      audioData: audioData,
      timestamp: Date.now(),
      delayMs: delayMs
    });

    // Prevent buffer overflow
    if (buffer.length > this.maxBufferSize) {
      const dropped = buffer.shift();
      console.warn(`[Buffer] âš  Buffer overflow for ${extension}, dropped packet (delay: ${dropped.delayMs}ms)`);
    }

    console.log(`[Buffer] ${extension} queued packet (delay: ${delayMs}ms, buffer size: ${buffer.length})`);
  }

  scheduleDelivery(extension, audioData, delayMs, deliveryCallback) {
    if (delayMs <= 0) {
      // No delay needed, deliver immediately
      deliveryCallback(audioData);
      console.log(`[Buffer] ${extension} delivered immediately (no delay needed)`);
      return;
    }

    // Add to buffer and schedule delayed delivery
    this.addToBuffer(extension, audioData, delayMs);

    setTimeout(() => {
      // Remove from buffer
      const buffer = this.buffers.get(extension);
      if (buffer && buffer.length > 0) {
        const packet = buffer.shift();
        const actualDelay = Date.now() - packet.timestamp;
        console.log(`[Buffer] ${extension} delivered after ${actualDelay}ms (target: ${delayMs}ms, drift: ${Math.abs(actualDelay - delayMs)}ms)`);
        deliveryCallback(packet.audioData);
      }
    }, delayMs);
  }

  getBufferSize(extension) {
    const buffer = this.buffers.get(extension);
    return buffer ? buffer.length : 0;
  }

  clearBuffer(extension) {
    if (this.buffers.has(extension)) {
      const size = this.buffers.get(extension).length;
      this.buffers.delete(extension);
      console.log(`[Buffer] âœ“ Cleared buffer for ${extension} (${size} packets dropped)`);
    }
  }

  getStats(extension) {
    const buffer = this.buffers.get(extension) || [];
    if (buffer.length === 0) {
      return { size: 0, avgDelay: 0, maxDelay: 0 };
    }

    const delays = buffer.map(p => p.delayMs);
    const avgDelay = delays.reduce((a, b) => a + b) / delays.length;
    const maxDelay = Math.max(...delays);

    return {
      size: buffer.length,
      avgDelay: Math.round(avgDelay),
      maxDelay: maxDelay
    };
  }
}

// Initialize AudioBufferManager globally
global.audioBufferManager = new AudioBufferManager();

// ============================================================================
// TIMING & BUFFERING MODULE - PHASE 6 & 7: Audio Routing & Conversion
// ============================================================================

// Global Map to track gateway sockets by extension (for bidirectional audio routing)
const gatewaySocketMap = new Map(); // extension â†’ socket

/**
 * Convert MP3 buffer to PCM using ffmpeg
 * @param {Buffer} mp3Buffer - Input MP3 audio buffer
 * @returns {Promise<Buffer>} - Output PCM buffer (16-bit, 8000Hz, mono)
 */
async function convertMp3ToPcm(mp3Buffer) {
  const { spawn } = require('child_process');

  return new Promise((resolve, reject) => {
    const ffmpeg = spawn('ffmpeg', [
      '-i', 'pipe:0',          // Input from stdin
      '-f', 's16le',           // Output format: signed 16-bit little-endian PCM
      '-ar', '8000',           // Sample rate: 8000 Hz (for telephony)
      '-ac', '1',              // Channels: mono
      'pipe:1'                 // Output to stdout
    ]);

    const chunks = [];

    ffmpeg.stdout.on('data', (chunk) => {
      chunks.push(chunk);
    });

    ffmpeg.stderr.on('data', (data) => {
      // Suppress ffmpeg stderr output (contains encoding progress)
    });

    ffmpeg.on('close', (code) => {
      if (code === 0) {
        resolve(Buffer.concat(chunks));
      } else {
        reject(new Error(`ffmpeg exited with code ${code}`));
      }
    });

    ffmpeg.on('error', (err) => {
      reject(err);
    });

    // Write MP3 buffer to ffmpeg stdin
    ffmpeg.stdin.write(mp3Buffer);
    ffmpeg.stdin.end();
  });
}

console.log('[Audio Routing] âœ“ MP3-to-PCM conversion function loaded');

// ============================================================================
// TIMING & BUFFERING MODULE - PHASE 4: DashboardTCPAPI
// ============================================================================

const net = require('net');

/**
 * DashboardTCPAPI - TCP server for dashboard latency queries
 * Listens on port 6001 for JSON commands and responses
 */
class DashboardTCPAPI {
  constructor(port = 6001) {
    this.port = port;
    this.server = null;
    this.clients = new Set();
  }

  start() {
    this.server = net.createServer((socket) => {
      this.clients.add(socket);
      console.log(`[TCP API] Client connected from ${socket.remoteAddress}:${socket.remotePort} (total: ${this.clients.size})`);

      let buffer = '';

      socket.on('data', (data) => {
        buffer += data.toString();

        // Process complete JSON messages (newline-delimited)
        let newlineIndex;
        while ((newlineIndex = buffer.indexOf('\n')) !== -1) {
          const message = buffer.substring(0, newlineIndex);
          buffer = buffer.substring(newlineIndex + 1);

          if (message.trim()) {
            this.handleMessage(socket, message);
          }
        }
      });

      socket.on('end', () => {
        this.clients.delete(socket);
        console.log(`[TCP API] Client disconnected (total: ${this.clients.size})`);
      });

      socket.on('error', (err) => {
        console.error(`[TCP API] Socket error: ${err.message}`);
        this.clients.delete(socket);
      });
    });

    this.server.listen(this.port, () => {
      console.log(`[TCP API] âœ“ Dashboard TCP API listening on port ${this.port}`);
    });

    this.server.on('error', (err) => {
      console.error(`[TCP API] âœ— Server error: ${err.message}`);
    });
  }

  handleMessage(socket, message) {
    try {
      const request = JSON.parse(message);
      console.log(`[TCP API] Request: ${request.command}`);

      let response = {};

      switch (request.command) {
        case 'getLatencies':
          response = this.getLatencies(request.extension);
          break;

        case 'getBufferStats':
          response = this.getBufferStats(request.extension);
          break;

        case 'getPairInfo':
          response = this.getPairInfo(request.extension);
          break;

        case 'ping':
          response = { status: 'ok', timestamp: Date.now() };
          break;

        default:
          response = { error: 'Unknown command', command: request.command };
      }

      socket.write(JSON.stringify(response) + '\n');
    } catch (err) {
      const errorResponse = { error: 'Invalid JSON', message: err.message };
      socket.write(JSON.stringify(errorResponse) + '\n');
    }
  }

  getLatencies(extension) {
    if (!extension) {
      return { error: 'Extension parameter required' };
    }

    if (!global.latencyTracker) {
      return { error: 'LatencyTracker not initialized' };
    }

    const latencies = global.latencyTracker.getAllLatencies(extension);
    const pairedExt = global.pairManager?.getPairedExtension(extension);

    let latencyDiff = 0;
    if (pairedExt) {
      const direction1 = `${extension}â†’${pairedExt}`;
      const direction2 = `${pairedExt}â†’${extension}`;
      latencyDiff = global.latencyTracker.getLatencyDifference(extension, pairedExt);
    }

    return {
      extension,
      pairedExtension: pairedExt || null,
      latencies,
      latencyDifference: latencyDiff,
      timestamp: Date.now()
    };
  }

  getBufferStats(extension) {
    if (!extension) {
      return { error: 'Extension parameter required' };
    }

    if (!global.audioBufferManager) {
      return { error: 'AudioBufferManager not initialized' };
    }

    const stats = global.audioBufferManager.getStats(extension);

    return {
      extension,
      bufferStats: stats,
      timestamp: Date.now()
    };
  }

  getPairInfo(extension) {
    if (!extension) {
      return { error: 'Extension parameter required' };
    }

    if (!global.pairManager) {
      return { error: 'PairManager not initialized' };
    }

    const isPaired = global.pairManager.isPaired(extension);
    const pairedExt = global.pairManager.getPairedExtension(extension);
    const callDuration = global.pairManager.getCallDuration(extension);

    return {
      extension,
      isPaired,
      pairedExtension: pairedExt || null,
      callDuration,
      timestamp: Date.now()
    };
  }

  broadcast(data) {
    const message = JSON.stringify(data) + '\n';
    this.clients.forEach(client => {
      try {
        client.write(message);
      } catch (err) {
        console.error(`[TCP API] Broadcast error: ${err.message}`);
        this.clients.delete(client);
      }
    });
  }

  stop() {
    this.clients.forEach(client => client.end());
    this.clients.clear();
    if (this.server) {
      this.server.close(() => {
        console.log('[TCP API] Server stopped');
      });
    }
  }
}

// Initialize and start DashboardTCPAPI (port 6002 to avoid conflict with bidirectional-timing-server on 6001)
global.dashboardTCPAPI = new DashboardTCPAPI(6002);
global.dashboardTCPAPI.start();

// ============================================================================

// Initialize Timing Server Client for bidirectional translation
const TimingClient = require('./timing-client');
global.timingClient = new TimingClient();
global.timingClient.connect().then(() => {
    console.log('[Server] âœ“ Timing client connected');
}).catch(err => {
    console.error('[Server] âœ— Timing client connection failed:', err.message);
});

// Phase 2: Global session registry for audio injection by extension
// Key: extension number (string), Value: session object
global.activeSessions = new Map();
console.log('[Phase2] Global session registry initialized');

// [DISABLED FOR 7777/8888] // Start AudioSocket server (for Asterisk integration on port 5050)
// [DISABLED FOR 7777/8888] // IMPORTANT: Must load AFTER global.io is set
// [DISABLED FOR 7777/8888] require("./audiosocket-integration");

// Phase 2: Set up INJECT_AUDIO handler for bidirectional audio buffering
global.timingClient.setInjectAudioHandler((msg) => {
    const { toExtension, audioData, timestamp } = msg;

    // Look up session by extension
    const session = global.activeSessions.get(String(toExtension));

    if (!session) {
        console.warn(`[Phase2] âœ— No session found for extension ${toExtension}`);
        return;
    }

    if (!session.micWebSocket || session.micWebSocket.readyState !== 1) {
        console.warn(`[Phase2] âœ— MicWebSocket not ready for extension ${toExtension}`);
        return;
    }

    // Decode base64 audio to buffer
    const audioBuffer = Buffer.from(audioData, 'base64');

    // Inject audio using the global function
    if (global.sendAudioToMicEndpoint) {
        global.sendAudioToMicEndpoint(session.micWebSocket, audioBuffer);
        console.log(`[Phase2] âœ“ Injected ${audioBuffer.length} bytes to extension ${toExtension}`);
    } else {
        console.error('[Phase2] âœ— sendAudioToMicEndpoint not available');
    }
});
console.log('[Phase2] INJECT_AUDIO handler registered');

// Latency Control Backend (for testing UI only - does not affect production)
// const LatencyControlBackend = require('./latency-control-backend');
// const latencyControl = new LatencyControlBackend();
// latencyControl.registerSocketHandlers(io);
console.log('[Server] âœ“ Latency Control Backend initialized (testing mode)');

app.use(express.static(path.join(__dirname, 'public')));

// Serve file directories
app.use('/files/recordings', express.static(path.join(__dirname, 'recordings')));
app.use('/files/transcripts', express.static(path.join(__dirname, 'transcripts')));
app.use('/files/translations', express.static(path.join(__dirname, 'translations')));

// Initialize services
const deepgramApiKey = process.env.DEEPGRAM_API_KEY;
const deeplApiKey = process.env.DEEPL_API_KEY;
const elevenlabsApiKey = process.env.ELEVENLABS_API_KEY;
const elevenlabsVoiceId = process.env.ELEVENLABS_DEFAULT_VOICE_ID;
const humeApiKey = process.env.HUME_EVI_API_KEY;

// Initialize DeepL translator
let translator;
if (deeplApiKey) {
  translator = new deepl.Translator(deeplApiKey);
}

// Initialize ElevenLabs TTS
let elevenlabsTTS = null;
if (elevenlabsApiKey) {
  elevenlabsTTS = new ElevenLabsTTSService(elevenlabsApiKey);
  console.log('ElevenLabs TTS service initialized');
}

// Store active rooms and participants
const rooms = new Map();
const participants = new Map();

// Store user profiles for HMLCP
const userProfiles = new Map(); // key: userId_language, value: { profile, uloLayer }

// QA Settings: Per-extension language configuration
// Extension 7777: English â†’ French (DEFAULT)
// Extension 7888: French â†’ English (OPPOSITE - for bidirectional translation)
global.qaConfigs = new Map();
global.qaConfigs.set('7777', { sourceLang: 'en', targetLang: 'fr', qaMode: false });
global.qaConfigs.set('7888', { sourceLang: 'fr', targetLang: 'en', qaMode: false });

// Helper function to get config for extension (with fallback)
function getQaConfig(extension) {
  return global.qaConfigs.get(extension) || global.qaConfigs.get('7777');
}

// Store streaming Deepgram connections per socket
const streamingConnections = new Map(); // key: socket.id, value: { connection, customVocab }
// Per-extension audio gain factors (key: extension, value: gainFactor)
const extensionGainFactors = new Map(); // Default 1.2x for all
const humeConnections = new Map(); // key: socket.id, value: HumeStreamingClient instance
const humeAudioBuffers = new Map(); // key: socket.id, value: array of audio chunks to buffer before sending to Hume
const socketToExtension = new Map(); // key: socket.id, value: extension (for Hume emotion events)
const HUME_BUFFER_SIZE = 50; // 50 chunks = ~1 second at 20ms per chunk

// Language mapping for services
const languageMap = {
  'en': { name: 'English', deepgram: 'en-US', deepl: 'en-US', azure: 'en-US' },
  'es': { name: 'Spanish', deepgram: 'es', deepl: 'ES', azure: 'es-ES' },
  'fr': { name: 'French', deepgram: 'fr', deepl: 'FR', azure: 'fr-FR' },
  'de': { name: 'German', deepgram: 'de', deepl: 'DE', azure: 'de-DE' },
  'it': { name: 'Italian', deepgram: 'it', deepl: 'IT', azure: 'it-IT' },
  'pt': { name: 'Portuguese', deepgram: 'pt', deepl: 'PT-PT', azure: 'pt-PT' },
  'ja': { name: 'Japanese', deepgram: 'ja', deepl: 'JA', azure: 'ja-JP' },
  'ko': { name: 'Korean', deepgram: 'ko', deepl: 'KO', azure: 'ko-KR' },
  'zh': { name: 'Chinese', deepgram: 'zh', deepl: 'ZH', azure: 'zh-CN' },
  'ru': { name: 'Russian', deepgram: 'ru', deepl: 'RU', azure: 'ru-RU' }
};

// Deepgram STT function with HMLCP custom vocabulary support
// Create WAV header for raw PCM audio so Deepgram can process it
function createWavHeader(pcmData) {
  const pcmLength = pcmData.length;
  const header = Buffer.alloc(44);
  header.write("RIFF", 0);
  header.writeUInt32LE(36 + pcmLength, 4);
  header.write("WAVE", 8);
  header.write("fmt ", 12);
  header.writeUInt32LE(16, 16);
  header.writeUInt16LE(1, 20);
  header.writeUInt16LE(1, 22);
  header.writeUInt32LE(16000, 24);
  header.writeUInt32LE(32000, 28);
  header.writeUInt16LE(2, 32);
  header.writeUInt16LE(16, 34);
  header.write("data", 36);
  header.writeUInt32LE(pcmLength, 40);
  return Buffer.concat([header, pcmData]);
}

// ========================================
// Audio Amplifier (fixes low volume issue)
// ========================================
function amplifyAudio(pcmBuffer, gainFactor = 1.0) {
  const amplified = Buffer.alloc(pcmBuffer.length);
  let maxSample = 0;
  let clippedSamples = 0;
  
  for (let i = 0; i < pcmBuffer.length; i += 2) {
    let sample = pcmBuffer.readInt16LE(i);
    maxSample = Math.max(maxSample, Math.abs(sample));
    
    let amplifiedSample = Math.round(sample * gainFactor);
    
    if (amplifiedSample > 32767) {
      amplifiedSample = 32767;
      clippedSamples++;
    } else if (amplifiedSample < -32768) {
      amplifiedSample = -32768;
      clippedSamples++;
    }
    
    amplified.writeInt16LE(amplifiedSample, i);
  }
  
  console.log(`[Audio Amplifier] Gain: ${gainFactor}x, Max input: ${maxSample}, Clipped: ${clippedSamples} samples (${(clippedSamples/(pcmBuffer.length/2)*100).toFixed(2)}%)`);
  return amplified;
}

async function transcribeAudio(audioBuffer, language, customVocab = []) {
  if (!deepgramApiKey) {
    console.warn('Deepgram API key not set');
    return { text: '[STT not configured]', confidence: 0 };
  }

  try {
    console.log(`[Deepgram] Starting transcription:`);
    console.log(`  - Audio buffer size: ${audioBuffer.length} bytes`);
    console.log(`  - Language: ${language}`);
    console.log(`  - Deepgram language code: ${languageMap[language]?.deepgram || 'en-US'}`);

    const deepgram = createClient(deepgramApiKey);

    // Build Deepgram options
    const options = {
      model: 'nova-2',
      language: languageMap[language]?.deepgram || 'en-US',
      smart_format: true,
      punctuate: true,
      utterances: false
    };

    // Add HMLCP custom vocabulary if provided
    if (customVocab && customVocab.length > 0) {
      // Deepgram keywords format: "phrase:boost"
      options.keywords = customVocab.map(v => `${v.phrase}:${v.boost}`);
      console.log(`[HMLCP] Using ${customVocab.length} custom vocabulary terms for STT`);
    }

    console.log(`[Deepgram] Options:`, JSON.stringify(options, null, 2));

    // Amplify audio before sending to Deepgram (fixes low volume issue)
    const amplifiedAudio = audioBuffer;
    const { result, error } = await deepgram.listen.prerecorded.transcribeFile(createWavHeader(amplifiedAudio), options);

    if (error) {
      console.error('[Deepgram] API returned error:');
      console.error('  Error object:', JSON.stringify(error, null, 2));
      return { text: '', confidence: 0 };
    }

    console.log('[Deepgram] Raw result structure:');
    console.log('  Has result:', !!result);
    console.log('  Has results:', !!result?.results);
    console.log('  Has channels:', !!result?.results?.channels);
    console.log('  Channels length:', result?.results?.channels?.length || 0);

    if (result?.results?.channels && result.results.channels.length > 0) {
      console.log('  Channel[0] alternatives:', result.results.channels[0]?.alternatives?.length || 0);

      if (result.results.channels[0]?.alternatives && result.results.channels[0].alternatives.length > 0) {
        const alt = result.results.channels[0].alternatives[0];
        console.log('  Transcript:', alt.transcript);
        console.log('  Confidence:', alt.confidence);
        console.log('  Words count:', alt.words?.length || 0);
      } else {
        console.log('[Deepgram] No alternatives in channel[0]');
      }
    } else {
      console.log('[Deepgram] No channels in result');
      console.log('[Deepgram] Full result:', JSON.stringify(result, null, 2));
    }

    const transcript = result?.results?.channels[0]?.alternatives[0]?.transcript || '';
    const confidence = result?.results?.channels[0]?.alternatives[0]?.confidence || 0;

    if (!transcript || transcript.trim() === '') {
      console.log('[Deepgram] Empty transcription returned');
    } else {
      console.log(`[Deepgram] SUCCESS: "${transcript}" (confidence: ${confidence})`);
    }

    return { text: transcript, confidence };
  } catch (error) {
    console.error('[Deepgram] Exception during transcription:');
    console.error('  Error message:', error.message);
    console.error('  Error stack:', error.stack);
    console.error('  Error object:', JSON.stringify(error, Object.getOwnPropertyNames(error), 2));
    return { text: '', confidence: 0 };
  }
}


// DeepL translation function
async function translateText(text, sourceLang, targetLang) {
  if (!translator) {
    console.warn('DeepL not configured');
    return `[Translation: ${text}]`;
  }

  if (!text || text.trim() === '') {
    return text;
  }

  // QA Mode: Override languages with qaConfig if QA mode is enabled
  if (global.qaConfig && (global.qaConfig.sourceLang || global.qaConfig.targetLang)) {
    const originalSource = sourceLang;
    const originalTarget = targetLang;
    
    // Override with QA config languages
    sourceLang = global.qaConfig.sourceLang || sourceLang;
    targetLang = global.qaConfig.targetLang || targetLang;
    
    console.log(`[QA Config] Language override: ${originalSource} â†’ ${originalTarget} becomes ${sourceLang} â†’ ${targetLang}`);
  }

  // Skip translation if source === target (applies to both QA mode and normal mode)
  if (sourceLang === targetLang) {
    console.log(`[Translation] Bypassed: ${sourceLang} â†’ ${targetLang} (same language)`);
    return text;
  }

  try {
    const sourceCode = languageMap[sourceLang]?.deepl || 'en-US';
    const targetCode = languageMap[targetLang]?.deepl || 'en-US';

    console.log(`[Translation] ${sourceLang} â†’ ${targetLang}: "${text.substring(0, 50)}..."`);

    const result = await translator.translateText(
      text,
      sourceCode === 'en-US' ? null : sourceCode,
      targetCode
    );

    return result.text;
  } catch (error) {
    console.error('Translation error:', error);
    return text;
  }
}
async function synthesizeSpeech(text, language) {
  if (!elevenlabsTTS) {
    console.warn('ElevenLabs TTS not configured');
    return null;
  }

  try {
    // Use the default voice ID from .env
    const voiceId = elevenlabsVoiceId || 'XPwQNE5RX9Rdhyx0DWcI'; // Boyan Tiholov

    // Synthesize using ElevenLabs (returns MP3 buffer)
    const result = await elevenlabsTTS.synthesize(text, voiceId, {
      modelId: 'eleven_multilingual_v2' // Supports 29 languages
    });

    if (result && result.audio) {
      return result.audio; // Return the audio buffer
    }

    return null;
  } catch (error) {
    console.error('ElevenLabs TTS error:', error);
    return null;
  }
}

// HMLCP: Get or create user profile with ULO layer
async function getUserProfile(userId, language) {
  const key = `${userId}_${language}`;

  if (!userProfiles.has(key)) {
    try {
      // Try to load existing profile or create new one
      const profile = await UserProfile.load(userId, language);
      const uloLayer = new ULOLayer(profile);
      const patternExtractor = new PatternExtractor();

      userProfiles.set(key, { profile, uloLayer, patternExtractor });
      console.log(`[HMLCP] Loaded profile for ${userId} (${language})`);
    } catch (error) {
      console.error(`[HMLCP] Error loading profile for ${userId}:`, error);
      // Create new profile on error
      const profile = new UserProfile(userId, language);
      const uloLayer = new ULOLayer(profile);
      const patternExtractor = new PatternExtractor();
      userProfiles.set(key, { profile, uloLayer, patternExtractor });
    }
  }

  return userProfiles.get(key);
}

// Create streaming Deepgram connection for real-time STT
async function createStreamingConnection(socket, participant) {
  if (!deepgramApiKey) {
    console.warn('[Streaming STT] Deepgram API key not set');
    return null;
  }

  // Get user profile and custom vocabulary for HMLCP BEFORE creating connection
  const { profile, uloLayer, patternExtractor } = await getUserProfile(
    participant.username,
    participant.language
  );
  const customVocab = uloLayer.generateCustomVocabulary();

  const deepgram = createClient(deepgramApiKey);

  // Configure streaming options with HMLCP custom vocabulary
  const options = {
    model: 'nova-2',
    language: languageMap[participant.language]?.deepgram || 'en-US',
    smart_format: true,
    punctuate: true,
    interim_results: false,  // Only get final results for accuracy
    endpointing: 300,  // 300ms silence to finalize utterance (faster than 800ms)
    utterance_end_ms: 1000  // Max 1 second to close utterance
  };

  // Add HMLCP custom vocabulary if available
  if (customVocab && customVocab.length > 0) {
    options.keywords = customVocab.map(v => `${v.phrase}:${v.boost}`);
    console.log(`[Streaming STT] ${participant.username}: Using ${customVocab.length} custom vocabulary terms`);
  }

  // Create live streaming connection and attach error handler IMMEDIATELY
  let connection;
  try {
    connection = deepgram.listen.live(options);

    // CRITICAL: Attach error handler IMMEDIATELY in same tick to catch early errors
    connection.on('Error', (error) => {
      console.error(`[Streaming STT] ${participant.username}: Connection error:`, error.message);
      streamingConnections.delete(socket.id);

      // Clean up Hume AI connection
      const humeClient = humeConnections.get(socket.id);
      if (humeClient) {
        humeClient.disconnect();
        humeConnections.delete(socket.id);
      }
      humeAudioBuffers.delete(socket.id);
      socketToExtension.delete(socket.id);
      socket.emit('pipeline-log', {
        type: 'client',
        stage: 'error',
        message: `Streaming STT error - using batch mode`,
        timestamp: Date.now()
      });
    });

    // Increase max listeners to avoid warnings
    connection.setMaxListeners(20);

  } catch (error) {
    console.error(`[Streaming STT] ${participant.username}: Failed to create connection:`, error.message);
    return null;
  }

  // Handle connection close
  connection.on('Close', () => {
    console.log(`[Streaming STT] ${participant.username}: Connection closed`);
    streamingConnections.delete(socket.id);

      // Clean up Hume AI connection
      const humeClient = humeConnections.get(socket.id);
      if (humeClient) {
        humeClient.disconnect();
        humeConnections.delete(socket.id);
      }
      humeAudioBuffers.delete(socket.id);
      socketToExtension.delete(socket.id);
  });

  // Handle transcription results
  connection.on('Results', async (data) => {
    const transcript = data.channel?.alternatives[0]?.transcript;

    if (!transcript || transcript.trim() === '') {
      return;  // Skip empty transcriptions
    }

    const confidence = data.channel?.alternatives[0]?.confidence || 0;
    const isFinal = data.is_final;

    // Only process final results
    if (isFinal) {
      const startTime = Date.now();
      console.log(`[Streaming STT] ${participant.username}: "${transcript}" (confidence: ${(confidence * 100).toFixed(1)}%)`);

      // Log STT complete
      socket.emit('pipeline-log', {
        type: 'client',
        stage: 'stt-complete',
        message: `Speech recognized: "${transcript}"`,
        timestamp: Date.now()
      });

      // HMLCP: Apply ULO layer for personalized processing
      const processedTranscription = uloLayer.apply(transcript);

      // Store sample for learning
      profile.addTextSample(transcript);

      // Log ULO processing if modified
      if (processedTranscription !== transcript) {
        console.log(`[HMLCP] ${participant.username}: ULO applied: "${transcript}" â†’ "${processedTranscription}"`);
        socket.emit('pipeline-log', {
          type: 'client',
          stage: 'hmlcp-ulo',
          message: `Personalized processing applied`,
          timestamp: Date.now()
        });
      }

      const finalTranscription = processedTranscription;

      // Send transcription to speaker
      socket.emit('transcription-result', {
        text: finalTranscription,
        rawText: transcript,
        confidence,
        language: participant.language
      });

      // Get room participants for translation
      const room = rooms.get(participant.roomId);
      if (!room) return;

      // Translate and synthesize for each other participant
      const translationPromises = Array.from(room.participants)
        .map(participantId => participants.get(participantId))
        .filter(p => {
          if (!p || p.id === socket.id) return false;  // Skip speaker
          // ECHO PREVENTION: Skip if target has same language (no translation needed)
          if (p.language === participant.language) {
            console.log(`[Echo Prevention] Skipping ${p.username} - same language as ${participant.username} (${participant.language})`);
            return false;
          }
          return true;
        })
        .map(async (targetParticipant) => {
          try {
            const transStart = Date.now();

            // Translate
            const translatedText = await translateText(
              finalTranscription,
              participant.language,
              targetParticipant.language
            );

            const transEnd = Date.now();
            const transDuration = transEnd - transStart;

            // TTS
            const ttsStart = Date.now();
            const audioData = await synthesizeSpeech(
              translatedText,
              targetParticipant.language
            );

            const ttsEnd = Date.now();
            const ttsDuration = ttsEnd - ttsStart;
            const totalLatency = Date.now() - startTime;

            // Calculate STT time as the remainder (STT happened before startTime was set)
            // In streaming mode, STT time is from audio send to transcript receipt
            // We approximate it as total - (translation + tts)
            const sttDuration = totalLatency - transDuration - ttsDuration;

            console.log(`[Streaming] ${participant.username} â†’ ${targetParticipant.username}: ${totalLatency}ms total (STT: ${sttDuration}ms, Trans: ${transDuration}ms, TTS: ${ttsDuration}ms)`);

            // Send to target participant
            io.to(targetParticipant.id).emit('translated-audio', {
              originalText: finalTranscription,
              rawTranscription: transcript,
              translatedText,
              audioData: audioData ? audioData.toString('base64') : null,
              speakerUsername: participant.username,
              speakerLanguage: participant.language,
              latency: totalLatency,
              timing: {
                stt: sttDuration,
                translation: transDuration,
                tts: ttsDuration,
                total: totalLatency
              }
            });

            io.to(targetParticipant.id).emit('pipeline-log', {
              type: 'client',
              stage: 'complete',
              message: `âœ“ Complete: ${totalLatency}ms (Trans: ${transDuration}ms, TTS: ${ttsDuration}ms)`,
              timestamp: Date.now()
            });

          } catch (error) {
            console.error(`[Streaming] ${participant.username} â†’ ${targetParticipant.username}: Error:`, error);
          }
        });

      await Promise.all(translationPromises);

      socket.emit('pipeline-log', {
        type: 'client',
        stage: 'speaker-confirmed',
        message: `Your speech sent to ${translationPromises.length} participant(s)`,
        timestamp: Date.now()
      });
    }
  });

  // Open the connection
  connection.on('Open', () => {
    console.log(`[Streaming STT] ${participant.username}: Connection opened successfully`);
    socket.emit('pipeline-log', {
      type: 'client',
      stage: 'info',
      message: `âœ“ Streaming mode active (low-latency)`,
      timestamp: Date.now()
    });
  });

  // Store connection
  streamingConnections.set(socket.id, {

    connection,
    customVocab,
    profile,
    uloLayer,
    patternExtractor
  });

  // Initialize Hume AI client for emotion detection
  initializeHumeClient(socket.id).catch(err => console.error("[Hume] Init error:", err));
  console.log(`[Streaming STT] ${participant.username}: Streaming connection created`);
  return connection;
}

// Socket.io connection handling
// Initialize Hume AI client for a socket
async function initializeHumeClient(socketId) {
  if (!humeApiKey) {
    console.log('[Hume] API key not configured');
    return null;
  }
  
  try {
    const humeClient = new HumeStreamingClient(humeApiKey, {
      sampleRate: 16000,
      channels: 1
    });
    
    await humeClient.connect();
    
    // Handle metrics from Hume
    console.log('[DEBUG] Attaching metrics listener for socket:', socketId);
    humeClient.on('metrics', (metrics) => {
      console.log('[DEBUG] ðŸ“Š Metrics event received from Hume:', {
        arousal: metrics.arousal,
        valence: metrics.valence,
        energy: metrics.energy,
        socketId: socketId
      });
      
      // Get extension for this socket
      const extension = socketToExtension.get(socketId);

      if (extension) {
        console.log('[DEBUG] âœ… Broadcasting emotion_detected to all clients for extension:', extension);
        // Broadcast to ALL clients (including dashboard) - similar to how TTS works
        global.io.emit('emotion_detected', {
          extension: String(extension), // Add extension field for filtering
          arousal: metrics.arousal,
          valence: metrics.valence,
          energy: metrics.energy,
          timestamp: metrics.timestamp
        });
        console.log('[DEBUG] Emotion event broadcasted successfully');
      } else {
        console.error('[DEBUG] âŒ Extension not found for socketId:', socketId);
      }
    });
    
    humeConnections.set(socketId, humeClient);
    humeAudioBuffers.set(socketId, []); // Initialize empty buffer for this socket
    console.log(`[Hume] Client initialized for socket ${socketId}`);
    return humeClient;
  } catch (error) {
    console.error('[Hume] Error initializing client:', error.message);
    return null;
  }
}

// ========================================
// PHASE 2D: Gateway Audio Buffer Tracking
// Track audio buffers per extension for AI pipeline processing
// ========================================

// Track audio buffers per extension
const gatewayAudioBuffers = new Map(); // key: extension, value: { chunks: [], totalBytes: 0 }

// Configuration
const BUFFER_SIZE_THRESHOLD = 60000; // ~0.9 seconds at 16kHz mono PCM (2 bytes per sample) - lowered to ensure processing triggers
/**
 * Add WAV header to raw PCM data
 * Format: 16kHz, mono, 16-bit PCM
 */
function addWavHeader(pcmBuffer) {
  const sampleRate = 16000;
  const numChannels = 1;
  const bitsPerSample = 16;
  const byteRate = sampleRate * numChannels * (bitsPerSample / 8);
  const blockAlign = numChannels * (bitsPerSample / 8);
  
  const wavHeader = Buffer.alloc(44);
  
  // RIFF chunk descriptor
  wavHeader.write('RIFF', 0);
  wavHeader.writeUInt32LE(36 + pcmBuffer.length, 4);
  wavHeader.write('WAVE', 8);
  
  // fmt sub-chunk
  wavHeader.write('fmt ', 12);
  wavHeader.writeUInt32LE(16, 16); // Subchunk1Size (16 for PCM)
  wavHeader.writeUInt16LE(1, 20); // AudioFormat (1 for PCM)
  wavHeader.writeUInt16LE(numChannels, 22);
  wavHeader.writeUInt32LE(sampleRate, 24);
  wavHeader.writeUInt32LE(byteRate, 28);
  wavHeader.writeUInt16LE(blockAlign, 32);
  wavHeader.writeUInt16LE(bitsPerSample, 34);
  
  // data sub-chunk
  wavHeader.write('data', 36);
  wavHeader.writeUInt32LE(pcmBuffer.length, 40);
  
  return Buffer.concat([wavHeader, pcmBuffer]);
}

/**
 * Process Gateway audio through AI pipeline
 * Called when buffer reaches ~1 second threshold
 */
async function processGatewayAudio(socket, extension, audioBuffer, language) {
  try {
    // ============================================================================
    // TIMING & BUFFERING MODULE - PHASE 5: Pipeline Timing Tracking
    // ============================================================================
    const pipelineStart = Date.now(); // Overall pipeline start time
    const timings = {}; // Store all stage timings

    // STAGE 1: Gateway â†’ ASR (audio processing and buffering)
    const gatewayToAsrStart = Date.now();
    console.log(`[Pipeline] Transcribing ${audioBuffer.length} bytes from extension ${extension}...`);
    // Amplify audio to improve Deepgram transcription accuracy
    // Use per-extension gain factor if set, otherwise default to 1.2
    const gainFactor = extensionGainFactors.get(extension) || 1.2;
    const amplifiedAudio = amplifyAudio(audioBuffer, gainFactor);
    // Add WAV header to raw PCM data for Deepgram
    const wavAudio = addWavHeader(amplifiedAudio);
    const gatewayToAsrEnd = Date.now();
    timings.gateway_to_asr = gatewayToAsrEnd - gatewayToAsrStart;
    global.latencyTracker.updateStageLatency(extension, 'gateway_to_asr', timings.gateway_to_asr);
    console.log(`[Timing] ${extension} | Stage 1/6: Gatewayâ†’ASR = ${timings.gateway_to_asr}ms`);

    // STAGE 2: ASR (Deepgram transcription)
    const asrStart = Date.now();
    const { text: transcription, confidence } = await transcribeAudio(wavAudio, language);
    const asrEnd = Date.now();
    timings.asr = asrEnd - asrStart;
    global.latencyTracker.updateStageLatency(extension, 'asr', timings.asr);
    console.log(`[Timing] ${extension} | Stage 2/6: ASR = ${timings.asr}ms`);

    if (!transcription || transcription.trim() === '') {
      console.log(`[Pipeline] No transcription for extension ${extension}`);
      return;
    }

    console.log(`[Pipeline] Transcription from ${extension}: "${transcription}" (confidence: ${confidence})`);

    // Emit transcription to dashboard
    global.io.emit('transcriptionFinal', {
      extension: extension,
      text: transcription,
      language: language,
      confidence: confidence,
      timestamp: Date.now()
    });

    // STAGE 3: ASR â†’ MT (text passing)
    const asrToMtStart = Date.now();
    const targetLang = extension === '7777' ? 'fr' : 'en'; // 7777 is English->French, 8888 is French->English
    console.log(`[Pipeline] Translating ${language} -> ${targetLang}: "${transcription}"`);
    const asrToMtEnd = Date.now();
    timings.asr_to_mt = asrToMtEnd - asrToMtStart;
    global.latencyTracker.updateStageLatency(extension, 'asr_to_mt', timings.asr_to_mt);
    console.log(`[Timing] ${extension} | Stage 3/6: ASRâ†’MT = ${timings.asr_to_mt}ms`);

    // STAGE 4: MT (Google Translate)
    const mtStart = Date.now();
    const translation = await translateText(transcription, language, targetLang);
    const mtEnd = Date.now();
    timings.mt = mtEnd - mtStart;
    global.latencyTracker.updateStageLatency(extension, 'mt', timings.mt);
    console.log(`[Timing] ${extension} | Stage 4/6: MT = ${timings.mt}ms`);
    console.log(`[Pipeline] Translation: "${translation}"`);

    // Emit translation to dashboard
    global.io.emit('translationComplete', {
      extension: extension,
      original: transcription,
      translation: translation,
      sourceLang: language,
      targetLang: targetLang,
      timestamp: Date.now()
    });

    // STAGE 5: MT â†’ TTS (text passing)
    const mtToTtsStart = Date.now();
    console.log(`[Pipeline] Generating TTS for extension ${extension}: "${translation}"`);
    const mtToTtsEnd = Date.now();
    timings.mt_to_tts = mtToTtsEnd - mtToTtsStart;
    global.latencyTracker.updateStageLatency(extension, 'mt_to_tts', timings.mt_to_tts);
    console.log(`[Timing] ${extension} | Stage 5/6: MTâ†’TTS = ${timings.mt_to_tts}ms`);

    // STAGE 6: TTS (ElevenLabs synthesis)
    const ttsStart = Date.now();
    const ttsAudio = await synthesizeSpeech(translation, targetLang);
    const ttsEnd = Date.now();
    timings.tts = ttsEnd - ttsStart;
    global.latencyTracker.updateStageLatency(extension, 'tts', timings.tts);
    console.log(`[Timing] ${extension} | Stage 6/6: TTS = ${timings.tts}ms`);

    // Calculate total serial pipeline latency
    const pipelineEnd = Date.now();
    const totalLatency = pipelineEnd - pipelineStart;
    timings.total = totalLatency;

    // Calculate serial pipeline sum for verification
    const serialSum = timings.gateway_to_asr + timings.asr + timings.asr_to_mt +
                       timings.mt + timings.mt_to_tts + timings.tts;

    console.log(`[Timing] ${extension} | TOTAL SERIAL PIPELINE: ${totalLatency}ms (sum: ${serialSum}ms)`);

    // Update direction latency for bidirectional comparison
    const pairedExt = global.pairManager.getPairedExtension(extension);
    if (pairedExt) {
      const direction = `${extension}â†’${pairedExt}`;
      global.latencyTracker.updateLatency(direction, totalLatency);
    }

    console.log(`[Pipeline] TTS generated: ${ttsAudio ? ttsAudio.length : 0} bytes (${timings.tts}ms)`);

    // Emit TTS audio to dashboard with full timing data
    console.log(`[TTS DEBUG] Emitting translated-audio for extension: ${extension} (type: ${typeof extension})`);
    global.io.emit('translated-audio', {
      extension: String(extension),
      original: transcription,
      translation: translation,
      audio: ttsAudio ? ttsAudio.toString('base64') : null,
      audioFormat: 'mp3',  // ElevenLabs returns MP3
      sourceLang: language,
      targetLang: targetLang,
      timestamp: Date.now(),
      timing: timings
    });

    // ===================================================================================
    // PHASE 7: BIDIRECTIONAL AUDIO ROUTING
    // Route translated audio to paired extension's gateway for true bidirectional translation
    // ===================================================================================
    if (ttsAudio) {
      const pairedExt = global.pairManager.getPairedExtension(extension);
      if (pairedExt) {
        const pairedSocket = gatewaySocketMap.get(pairedExt);
        if (pairedSocket && pairedSocket.connected) {
          try {
            console.log(`[Audio Routing] Converting MP3 to PCM for ${extension}â†’${pairedExt}`);

            // Convert MP3 to PCM (8000 Hz, 16-bit, mono) for gateway playback
            const pcmAudio = await convertMp3ToPcm(ttsAudio);

            console.log(`[Audio Routing] âœ“ Converted ${ttsAudio.length} bytes MP3 â†’ ${pcmAudio.length} bytes PCM`);

            // Emit to paired extension's gateway socket
            pairedSocket.emit('translatedAudio', {
              audio: pcmAudio,
              audioFormat: 'pcm',
              sampleRate: 8000,
              bitDepth: 16,
              channels: 1,
              sourceExtension: extension,
              targetExtension: pairedExt,
              timestamp: Date.now()
            });

            console.log(`[Audio Routing] âœ“ Sent ${pcmAudio.length} bytes PCM to extension ${pairedExt}`);
          } catch (conversionError) {
            console.error(`[Audio Routing] âœ— MP3-to-PCM conversion failed for ${extension}â†’${pairedExt}:`, conversionError.message);
          }
        } else {
          console.log(`[Audio Routing] âš  Paired extension ${pairedExt} gateway socket not connected`);
        }
      } else {
        console.log(`[Audio Routing] âš  No paired extension found for ${extension}`);
      }
    }

  } catch (error) {
    console.error(`[Pipeline] Error in processGatewayAudio for ${extension}:`, error);
  }
}



io.on('connection', (socket) => {
  console.log('Client connected:', socket.id);

  // ========================================
  // PHASE 2C: Gateway WebSocket Handlers
  // Handle events from ExternalMedia Gateway (extensions 7777/8888)
  // ========================================
  
  // Track Gateway connections by extension
  const gatewayConnections = socket.gatewayConnections || new Map();
  
  // Register extension from Gateway
  socket.on('registerExtension', (data) => {
    console.log(`[Gateway] Extension ${data.extension} registered (${data.language})`);

    // Store Gateway connection info
    gatewayConnections.set(data.extension, {
      extension: data.extension,
      language: data.language,
      format: data.format,
      sampleRate: data.sampleRate,
      socketId: socket.id
    });

    // Store reverse mapping: socketId -> extension (for Hume emotion events)
    socketToExtension.set(socket.id, data.extension);
    console.log(`[Gateway] Socket ${socket.id} mapped to extension ${data.extension}`);

    // PHASE 7: Register socket in global gateway socket map for audio routing
    gatewaySocketMap.set(data.extension, socket);
    console.log(`[Audio Routing] âœ“ Gateway socket registered for extension ${data.extension}`);

    // Initialize Hume AI client for emotion detection
    initializeHumeClient(socket.id).catch(err => console.error("[Hume] Init error:", err));

    socket.emit('registrationConfirmed', {
      extension: data.extension,
      status: 'ready'
    });
  });
  
  // Receive audio stream from Gateway
  socket.on('audioStream', async (data) => {
    const { extension, audio, timestamp } = data;
    
    // Initialize buffer for this extension if needed
    if (!gatewayAudioBuffers.has(extension)) {
      gatewayAudioBuffers.set(extension, {
        chunks: [],
        totalBytes: 0,
        language: gatewayConnections.get(extension)?.language || 'en'
      });
    }
    
    const buffer = gatewayAudioBuffers.get(extension);
    const audioChunk = Buffer.isBuffer(audio) ? audio : Buffer.from(audio);
    console.log("[RMS DEBUG] audioChunk.length:", audioChunk.length);
    console.log("[RMS DEBUG] audioChunk.length:", audioChunk.length);

    // Fork audio to Hume AI for emotion detection (buffered for better speech detection)
    const humeClient = humeConnections.get(socket.id);
    if (humeClient && humeClient.connected) {
      const humeBuffer = humeAudioBuffers.get(socket.id);
      if (humeBuffer) {
        humeBuffer.push(audioChunk);

        // Send to Hume only when buffer reaches threshold (50 chunks = ~1 second)
        if (humeBuffer.length >= HUME_BUFFER_SIZE) {
          const combinedBuffer = Buffer.concat(humeBuffer);
          humeClient.sendAudio(combinedBuffer);
          console.log(`[Hume] Sent ${HUME_BUFFER_SIZE} chunks (${combinedBuffer.length} bytes, ~1 second buffer)`);

          // Reset buffer for next batch
          humeAudioBuffers.set(socket.id, []);
        }
      }
    }

    // Calculate enhanced audio metrics (RMS, peak, clipping) for professional monitoring
    let rmsLevel = 0;
    let peakLevel = 0;
    let clippingCount = 0;

    if (audioChunk.length > 0) {
      let sumSquares = 0;
      let maxSample = 0;

      for (let i = 0; i < audioChunk.length; i += 2) {
        const sample = audioChunk.readInt16LE(i);
        const absSample = Math.abs(sample);

        // RMS calculation
        sumSquares += sample * sample;

        // Peak calculation
        if (absSample > maxSample) {
          maxSample = absSample;
        }

        // Clipping detection (samples at or near max int16 value)
        if (absSample >= 32767 || absSample >= 32500) {
          clippingCount++;
        }
      }

      const sampleCount = audioChunk.length / 2;
      const rms = Math.sqrt(sumSquares / sampleCount);
      console.log("[RMS DEBUG] sumSquares:", sumSquares, "sampleCount:", sampleCount, "rms:", rms, "rmsLevel:", Math.min(100, Math.round((rms / 32768) * 100)));
      console.log("[RMS DEBUG] sumSquares:", sumSquares, "sampleCount:", sampleCount, "rms:", rms, "rmsLevel:", Math.min(100, Math.round((rms / 32768) * 100)));

      // Normalize to 0-100 for RMS
      rmsLevel = Math.min(100, Math.round((rms / 32768) * 100));
      console.log("[RMS DEBUG] Final rmsLevel:", rmsLevel, "peakLevel:", peakLevel, "clippingCount:", clippingCount);

      // Normalize peak to 0-100
      peakLevel = Math.min(100, Math.round((maxSample / 32768) * 100));
    }

    // Emit monitoring event for Extension Monitor card with enhanced audio metrics
    global.io.emit('transcriptionPartial', {
      extension: extension,
      bytes: audioChunk.length,
      timestamp: timestamp || Date.now(),
      uuid: gatewayConnections.get(extension)?.socketId || socket.id,
      audioLevel: rmsLevel,
      peakLevel: peakLevel,
      clipping: clippingCount
    });

    // ============================================================================
    // REAL-TIME PROCESSING: Process audio chunks immediately (no input buffering)
    // Buffering happens on OUTPUT side for smooth playback delivery
    // ============================================================================

    // Add chunk to buffer for monitoring purposes only
    buffer.chunks.push(audioChunk);
    buffer.totalBytes += audioChunk.length;

    // Keep only last 5 chunks for monitoring (prevent memory leak)
    if (buffer.chunks.length > 5) {
      buffer.chunks.shift();
      buffer.totalBytes = buffer.chunks.reduce((sum, chunk) => sum + chunk.length, 0);
    }

    // Log every 50th packet to avoid flooding
    if (Math.random() < 0.02) {
      console.log(`[Gateway] Extension ${extension}: received ${audioChunk.length} bytes (processing immediately)`);
    }

    // Process audio chunk IMMEDIATELY through AI pipeline (async, don't wait)
    processGatewayAudio(socket, extension, audioChunk, buffer.language).catch(err => {
      console.error(`[Gateway] Error processing audio for ${extension}:`, err.message);
    });
  });
  // Join or create room
  socket.on('join-room', (data) => {
    const { roomId, username, language } = data;

    socket.join(roomId);

    // Store participant info
    participants.set(socket.id, {
      id: socket.id,
      username,
      language,
      roomId
    });

    // Initialize room if it doesn't exist
    if (!rooms.has(roomId)) {
      rooms.set(roomId, {
        id: roomId,
        participants: new Set()
      });
    }

    const room = rooms.get(roomId);
    room.participants.add(socket.id);

    console.log(`${username} joined room ${roomId} with language ${language}`);

    // Notify others in the room
    socket.to(roomId).emit('participant-joined', {
      participantId: socket.id,
      username,
      language
    });

    // Send current participants to the new user
    const currentParticipants = Array.from(room.participants)
      .filter(id => id !== socket.id)
      .map(id => participants.get(id))
      .filter(p => p);

    socket.emit('room-joined', {
      roomId,
      participants: currentParticipants
    });

    // Streaming STT temporarily disabled - API key authentication issues in production context
    // The standalone test passes, but server crashes with "non-101 status code" error
    // This suggests the API key works in isolation but not in server context
    // Falling back to stable batch mode (~1500-2400ms latency)
    const participant = participants.get(socket.id);
    if (participant) {
      createStreamingConnection(socket, participant)
        .then(connection => {
          if (connection) {
            console.log(`[Mode] âœ… Streaming mode enabled for ${participant.username}`);
          } else {
            console.log(`[Mode] âš ï¸ Streaming failed, falling back to batch mode for ${participant.username}`);
            socket.emit('pipeline-log', {
              type: 'client',
              stage: 'info',
              message: `âš ï¸ Batch mode active (streaming unavailable)`,
              timestamp: Date.now()
            });
          }
        })
        .catch(error => {
          console.error(`[Mode] Error creating streaming connection:`, error);
          socket.emit('pipeline-log', {
            type: 'client',
            stage: 'info',
            message: `âš ï¸ Batch mode active (streaming error)`,
            timestamp: Date.now()
          });
        });
    }
    
  });

  // Handle audio stream - feed to Deepgram Live connection or fallback to batch
  socket.on('audio-stream', async (data) => {
    const { audioBuffer, roomId } = data;
    const streamData = streamingConnections.get(socket.id);
    const participant = participants.get(socket.id);

    if (!participant) return;

    // Try streaming mode first
    if (streamData && streamData.connection) {
      try {
        // Send audio chunk to Deepgram Live API
        streamData.connection.send(Buffer.from(audioBuffer));

        // Fork audio to Hume AI for emotion detection
        const humeClient = humeConnections.get(socket.id);
        if (humeClient && humeClient.connected) {
          humeClient.sendAudio(Buffer.from(audioBuffer));
        }
      } catch (error) {
        console.error(`[Streaming] Error sending audio:`, error);
      }
    } else {
      // Fallback to batch mode if streaming not available
      // This prevents system from breaking if Deepgram Live API isn't available
      console.log(`[Batch Mode] Processing audio for ${participant.username} (streaming unavailable)`);

      try {
        const { profile, uloLayer } = await getUserProfile(
          participant.username,
          participant.language
        );
        const customVocab = uloLayer.generateCustomVocabulary();

        const startTime = Date.now();
        const sttStart = Date.now();
        const { text: transcription, confidence } = await transcribeAudio(
          Buffer.from(audioBuffer),
          participant.language,
          customVocab
        );

        const sttEnd = Date.now();
        const sttDuration = sttEnd - sttStart;

        if (!transcription || transcription.trim() === '') {
          return;
        }

        const processedTranscription = uloLayer.apply(transcription);
        profile.addTextSample(transcription);

        socket.emit('transcription-result', {
          text: processedTranscription,
          rawText: transcription,
          confidence,
          language: participant.language
        });

        // Translate for other participants
        const room = rooms.get(roomId);
        if (!room) return;

        const translationPromises = Array.from(room.participants)
          .map(participantId => participants.get(participantId))
          .filter(p => p && p.id !== socket.id)  // Skip speaker only
          .map(async (targetParticipant) => {
            const transStart = Date.now();
            const translatedText = await translateText(
              processedTranscription,
              participant.language,
              targetParticipant.language
            );

            const transEnd = Date.now();
            const transDuration = transEnd - transStart;

            const ttsStart = Date.now();
            const audioData = await synthesizeSpeech(
              translatedText,
              targetParticipant.language
            );

            const ttsEnd = Date.now();
            const ttsDuration = ttsEnd - ttsStart;
            const totalLatency = Date.now() - startTime;

            io.to(targetParticipant.id).emit('translated-audio', {
              originalText: processedTranscription,
              rawTranscription: transcription,
              translatedText,
              audioData: audioData ? audioData.toString('base64') : null,
              speakerUsername: participant.username,
              speakerLanguage: participant.language,
              latency: totalLatency,
              timing: {
                stt: sttDuration,
                translation: transDuration,
                tts: ttsDuration,
                total: totalLatency
              }
            });
          });

        await Promise.all(translationPromises);
      } catch (error) {
        console.error(`[Batch Mode] Error:`, error);
      }
    }
  });

  // Handle voice profile creation (from onboarding)
  socket.on('create-voice-profile', async (data) => {
    const { profileId, language, phrasesCount, skipped } = data;

    console.log(`[Onboarding] Creating voice profile: ${profileId} (${language}), skipped: ${skipped}`);

    try {
      // Initialize HMLCP profile for this user
      const { profile } = await getUserProfile(profileId, language);

      // If user skipped calibration, apply default language profile
      if (skipped || phrasesCount === 0) {
        console.log(`[Onboarding] Applying default ${language} profile (calibration skipped)`);
        applyDefaultProfile(profile, language);
        await profile.save();
      }

      socket.emit('profile-created', {
        profileId,
        language,
        success: true,
        isDefault: skipped || phrasesCount === 0
      });

      console.log(`[Onboarding] Profile created: ${profileId}${skipped ? ' (default profile)' : ''}`);
    } catch (error) {
      console.error('[Onboarding] Error creating profile:', error);
      socket.emit('error', { message: 'Failed to create profile' });
    }
  });

  // Handle calibration audio (from onboarding)
  socket.on('calibration-audio', async (data) => {
    const { audioBuffer, phrase, phraseIndex, language, profileId } = data;

    console.log(`[Onboarding] Calibration audio received: phrase ${phraseIndex + 1}, ${audioBuffer.byteLength} bytes`);

    try {
      // Transcribe the calibration audio
      const { text: transcription, confidence } = await transcribeAudio(
        Buffer.from(audioBuffer),
        language
      );

      console.log(`[Onboarding] Calibration phrase ${phraseIndex + 1} transcribed: "${transcription}" (expected: "${phrase}")`);

      // If we have a profile, add this as a calibration sample
      if (profileId) {
        const { profile, patternExtractor } = await getUserProfile(profileId, language);

        // Add to profile as a calibration sample
        profile.addTextSample(transcription);
        profile.addCalibrationSample(phrase, transcription);

        // Extract patterns if mismatch detected
        if (transcription.toLowerCase() !== phrase.toLowerCase()) {
          const pattern = patternExtractor.extractCorrectionPattern(transcription, phrase);
          if (pattern) {
            profile.addPattern(pattern);
            console.log(`[Onboarding] Pattern learned from calibration: ${transcription} â†’ ${phrase}`);
          }
        }

        // Save profile after each calibration phrase
        await profile.save();
      }

      // Notify client that processing is complete
      socket.emit('calibration-processed', {
        phraseIndex,
        transcription,
        expectedPhrase: phrase,
        confidence,
        success: true
      });

    } catch (error) {
      console.error('[Onboarding] Error processing calibration audio:', error);
      socket.emit('error', { message: 'Failed to process calibration audio' });
    }
  });

  // Handle disconnect

  // QA Settings: Handle language configuration from dashboard
  socket.on('qa-language-config', (config) => {
    const { sourceLang, targetLang, qaMode, extension } = config;

    if (!extension) {
      console.warn('[QA Config] âš ï¸  No extension specified, ignoring update');
      return;
    }

    // Update per-extension QA configuration
    global.qaConfigs.set(extension, { sourceLang, targetLang, qaMode });

    console.log(`[QA Config] âœ“ Extension ${extension} updated: ${sourceLang} â†’ ${targetLang} (QA Mode: ${qaMode})`);

    // Broadcast to all connected clients (they will filter by extension)
    io.emit('qa-config-updated', {
      extension,
      sourceLang,
      targetLang,
      qaMode
    });
  });

  // Handle volume control requests from dashboard
  socket.on('setVolume', (data) => {
    const { extension, volumeTX } = data;
    console.log(`[Volume Control] Request to set extension ${extension} TX gain to ${volumeTX} dB`);

    // Convert dB to linear gain factor: gainFactor = 10^(dB/20)
    const gainFactor = Math.pow(10, volumeTX / 20);

    // Store the gain factor for this extension
    extensionGainFactors.set(extension, gainFactor);

    console.log(`[Volume Control] âœ“ Extension ${extension} PCM gain set to ${gainFactor.toFixed(3)}x (${volumeTX} dB)`);
  });

  socket.on('disconnect', () => {
    const participant = participants.get(socket.id);

    // Close streaming Deepgram connection
    const streamData = streamingConnections.get(socket.id);
    if (streamData && streamData.connection) {
      try {
        streamData.connection.finish();
        console.log(`[Streaming STT] Connection closed for ${participant?.username || socket.id}`);
      } catch (error) {
        console.error(`[Streaming STT] Error closing connection:`, error);
      }
      streamingConnections.delete(socket.id);

      // Clean up Hume AI connection
      const humeClient = humeConnections.get(socket.id);
      if (humeClient) {
        humeClient.disconnect();
        humeConnections.delete(socket.id);
      }
      humeAudioBuffers.delete(socket.id);

      // PHASE 7: Remove from gateway socket map if it was a gateway
      const extension = socketToExtension.get(socket.id);
      if (extension) {
        gatewaySocketMap.delete(extension);
        console.log(`[Audio Routing] âœ“ Gateway socket removed for extension ${extension}`);
      }

      socketToExtension.delete(socket.id);
    }

    if (participant) {
      const { roomId, username } = participant;
      const room = rooms.get(roomId);

      if (room) {
        room.participants.delete(socket.id);

        // Notify others
        socket.to(roomId).emit('participant-left', {
          participantId: socket.id,
          username
        });

        // Clean up empty rooms
        if (room.participants.size === 0) {
          rooms.delete(roomId);
          console.log(`Room ${roomId} deleted (empty)`);
        }
      }

      participants.delete(socket.id);
      console.log(`${username} disconnected from room ${roomId}`);
    }
  });
});

// API Routes
app.get('/', (req, res) => {
  res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

app.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    services: {
      deepgram: !!deepgramApiKey,
      deepl: !!deeplApiKey,
      elevenlabs: !!elevenlabsApiKey
    },
    activeRooms: rooms.size,
    activeParticipants: participants.size
  });
});

app.get('/api/languages', (req, res) => {
  res.json(languageMap);
});

// File listing API endpoints
app.get('/api/files/recordings', (req, res) => {
  const recordingsDir = path.join(__dirname, 'recordings');
  fs.readdir(recordingsDir, (err, files) => {
    if (err) {
      return res.json({ files: [] });
    }
    const fileList = files.map(filename => ({
      name: filename,
      path: `/files/recordings/${filename}`
    }));
    res.json({ files: fileList });
  });
});

app.get('/api/files/transcripts', (req, res) => {
  const transcriptsDir = path.join(__dirname, 'transcripts');
  fs.readdir(transcriptsDir, (err, files) => {
    if (err) {
      return res.json({ files: [] });
    }
    const fileList = files.map(filename => ({
      name: filename,
      path: `/files/transcripts/${filename}`
    }));
    res.json({ files: fileList });
  });
});

app.get('/api/files/translations', (req, res) => {
  const translationsDir = path.join(__dirname, 'translations');
  fs.readdir(translationsDir, (err, files) => {
    if (err) {
      return res.json({ files: [] });
    }
    const fileList = files.map(filename => ({
      name: filename,
      path: `/files/translations/${filename}`
    }));
    res.json({ files: fileList });
  });
});

// HMLCP API Endpoints
app.use(express.json());

// Get user profile stats
app.get('/api/hmlcp/profile/:userId/:language', async (req, res) => {
  try {
    const { userId, language } = req.params;
    const { profile, uloLayer } = await getUserProfile(userId, language);

    res.json({
      userId: profile.userId,
      language: profile.language,
      created: profile.created,
      lastUpdated: profile.lastUpdated,
      tone: profile.tone,
      avgSentenceLength: profile.avgSentenceLength,
      directness: profile.directness,
      ambiguityTolerance: profile.ambiguityTolerance,
      lexicalBias: profile.lexicalBias,
      phraseMappings: Object.keys(profile.phraseMap).length,
      biasTerms: profile.biasTerms.length,
      samples: profile.textSamples.length + profile.audioSamples.length,
      corrections: profile.corrections.length,
      metrics: profile.metrics,
      uloStats: uloLayer.getStats()
    });
  } catch (error) {
    console.error('[HMLCP API] Error getting profile:', error);
    res.status(500).json({ error: 'Failed to get profile' });
  }
});

// Submit a correction (for learning)
app.post('/api/hmlcp/correction', async (req, res) => {
  try {
    const { userId, language, rawInput, interpretedIntent, correctedIntent } = req.body;

    if (!userId || !language || !rawInput || !correctedIntent) {
      return res.status(400).json({ error: 'Missing required fields' });
    }

    const { profile, uloLayer } = await getUserProfile(userId, language);

    // Learn from correction
    uloLayer.learnFromCorrection(rawInput, interpretedIntent || rawInput, correctedIntent);

    // Save profile
    await profile.save();

    res.json({
      success: true,
      message: 'Correction learned',
      metrics: profile.metrics,
      newPhraseMappings: Object.keys(profile.phraseMap).length
    });
  } catch (error) {
    console.error('[HMLCP API] Error submitting correction:', error);
    res.status(500).json({ error: 'Failed to submit correction' });
  }
});

// Analyze profile patterns
app.post('/api/hmlcp/analyze', async (req, res) => {
  try {
    const { userId, language } = req.body;

    if (!userId || !language) {
      return res.status(400).json({ error: 'Missing userId or language' });
    }

    const { profile, patternExtractor } = await getUserProfile(userId, language);

    // Extract patterns from collected samples
    const patterns = patternExtractor.analyze(profile.textSamples);

    // Update profile with extracted patterns
    profile.tone = patterns.tone;
    profile.avgSentenceLength = patterns.avgSentenceLength;
    profile.directness = patterns.directness;
    profile.ambiguityTolerance = patterns.ambiguityTolerance;
    profile.lexicalBias = patterns.lexicalBias;

    // Save updated profile
    await profile.save();

    res.json({
      success: true,
      patterns,
      message: 'Profile patterns analyzed and updated'
    });
  } catch (error) {
    console.error('[HMLCP API] Error analyzing patterns:', error);
    res.status(500).json({ error: 'Failed to analyze patterns' });
  }
});

// Get custom vocabulary for Deepgram
app.get('/api/hmlcp/vocabulary/:userId/:language', async (req, res) => {
  try {
    const { userId, language } = req.params;
    const { uloLayer } = await getUserProfile(userId, language);

    const vocabulary = uloLayer.generateCustomVocabulary();

    res.json({
      success: true,
      vocabulary,
      count: vocabulary.length
    });
  } catch (error) {
    console.error('[HMLCP API] Error getting vocabulary:', error);
    res.status(500).json({ error: 'Failed to get vocabulary' });
  }
});

// Save profile manually
app.post('/api/hmlcp/save', async (req, res) => {
  try {
    const { userId, language } = req.body;

    if (!userId || !language) {
      return res.status(400).json({ error: 'Missing userId or language' });
    }

    const { profile } = await getUserProfile(userId, language);
    const filePath = await profile.save();

    res.json({
      success: true,
      message: 'Profile saved',
      filePath
    });
  } catch (error) {
    console.error('[HMLCP API] Error saving profile:', error);
    res.status(500).json({ error: 'Failed to save profile' });
  }
});

const PORT = process.env.PORT || 3000;
const HOST = '0.0.0.0'; // Listen on all network interfaces

server.listen(PORT, HOST, () => {
  // Get local IP addresses
  const os = require('os');
  const networkInterfaces = os.networkInterfaces();
  const localIPs = [];

  Object.keys(networkInterfaces).forEach(interfaceName => {
    networkInterfaces[interfaceName].forEach(iface => {
      if (iface.family === 'IPv4' && !iface.internal) {
        localIPs.push(iface.address);
      }
    });
  });

  const protocol = server instanceof https.Server ? 'https' : 'http';

  console.log(`Conference server running on port ${PORT}`);
  console.log(`\nProtocol: ${protocol.toUpperCase()}`);
  console.log('\nAccess URLs:');
  console.log(`  - Local:    ${protocol}://localhost:${PORT}`);
  localIPs.forEach(ip => {
    console.log(`  - Network:  ${protocol}://${ip}:${PORT}`);
  });
  console.log('\nServices status:');
  console.log('  - Deepgram STT:', deepgramApiKey ? 'âœ“' : 'âœ— (not configured)');
  console.log('  - DeepL Translation:', deeplApiKey ? 'âœ“' : 'âœ— (not configured)');
  console.log('  - ElevenLabs TTS:', elevenlabsApiKey ? 'âœ“' : 'âœ— (not configured)');

  if (protocol === 'https') {
    console.log('\nâœ“ HTTPS enabled - Microphone will work on remote devices!');
    console.log('  (You may need to click "Advanced" and accept the self-signed certificate)');
  } else {
    console.log('\nâš  HTTP only - Microphone will only work on localhost');
    console.log('  Add cert.pem and key.pem for HTTPS support');
  }
  console.log('\nâœ“ Server is accessible from other devices on your network');

  // HMLCP: Periodic profile saving (every 5 minutes)
  setInterval(async () => {
    let savedCount = 0;
    for (const [key, { profile }] of userProfiles.entries()) {
      try {
        await profile.save();
        savedCount++;
      } catch (error) {
        console.error(`[HMLCP] Error saving profile ${key}:`, error.message);
      }
    }
    if (savedCount > 0) {
      console.log(`[HMLCP] Auto-saved ${savedCount} profile(s)`);
    }
  }, 5 * 60 * 1000); // 5 minutes

  console.log('\n[HMLCP] System initialized');
  console.log('  - User profiles: Auto-save enabled (every 5 min)');
  console.log('  - ULO Layer: Active for real-time adaptation');
  console.log('  - Pattern Extraction: Available via API');
});
